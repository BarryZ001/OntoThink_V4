#!/bin/bash
# ChatGLM3 å®Œæ•´ä¿®å¤å·¥å…· - å½»åº•è§£å†³tokenizerå’Œæ¨¡å‹æ–‡ä»¶é—®é¢˜
# é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ

set -e

echo "ğŸ”§ ChatGLM3 å®Œæ•´ä¿®å¤å·¥å…·"
echo "é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ"
echo "========================================"

# è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ONTOTHINK_ROOT="$SCRIPT_DIR"
MODEL_DIR="${ONTOTHINK_ROOT}/enflame_training/models/THUDM/chatglm3-6b"

echo "ğŸ“ é¡¹ç›®æ ¹ç›®å½•: $ONTOTHINK_ROOT"
echo "ğŸ“ æ¨¡å‹ç›®å½•: $MODEL_DIR"

# å½»åº•æ¸…ç†ç°æœ‰æ¨¡å‹
echo ""
echo "ğŸ§¹ å½»åº•æ¸…ç†ç°æœ‰æ¨¡å‹æ–‡ä»¶..."
if [ -d "$MODEL_DIR" ]; then
    echo "ğŸ—‘ï¸  åˆ é™¤ç°æœ‰æ¨¡å‹ç›®å½•..."
    rm -rf "$MODEL_DIR"
fi

# æ¸…ç†Hugging Faceç¼“å­˜
echo "ğŸ§¹ æ¸…ç†Hugging Faceç¼“å­˜..."
if [ -d "$HOME/.cache/huggingface" ]; then
    rm -rf "$HOME/.cache/huggingface"
    echo "âœ… Hugging Faceç¼“å­˜å·²æ¸…ç†"
fi

# æ¸…ç†transformersç¼“å­˜
echo "ğŸ§¹ æ¸…ç†transformersç¼“å­˜..."
if [ -d "$HOME/.cache/torch" ]; then
    rm -rf "$HOME/.cache/torch"
    echo "âœ… Torchç¼“å­˜å·²æ¸…ç†"
fi

# åˆ›å»ºæ¨¡å‹ç›®å½•
echo ""
echo "ğŸ“ åˆ›å»ºæ¨¡å‹ç›®å½•..."
mkdir -p "$MODEL_DIR"
cd "$MODEL_DIR"

echo "ğŸ“ å½“å‰ç›®å½•: $PWD"

# æ–¹æ³•1: ä½¿ç”¨git clone (æœ€å¯é çš„æ–¹æ³•)
echo ""
echo "ğŸ”„ æ–¹æ³•1: ä½¿ç”¨git cloneç›´æ¥ä¸‹è½½..."
echo "è¿™æ˜¯æœ€å¯é çš„æ–¹æ³•ï¼Œä¼šä¸‹è½½å®Œæ•´çš„ä»“åº“"

# è®¾ç½®Git LFSç¯å¢ƒ
export GIT_LFS_SKIP_SMUDGE=0

if git clone https://huggingface.co/THUDM/chatglm3-6b . --depth 1; then
    echo "âœ… Git cloneæˆåŠŸ"
    
    # æ£€æŸ¥Git LFS
    echo "ğŸ” æ£€æŸ¥Git LFSçŠ¶æ€..."
    if command -v git-lfs >/dev/null 2>&1; then
        echo "âœ… Git LFSå·²å®‰è£…"
        
        # ç¡®ä¿LFSæ–‡ä»¶ä¸‹è½½
        echo "ğŸ“¥ ç¡®ä¿LFSæ–‡ä»¶ä¸‹è½½..."
        git lfs pull
        
        # æ£€æŸ¥LFSçŠ¶æ€
        git lfs ls-files
    else
        echo "âš ï¸  Git LFSæœªå®‰è£…ï¼Œå°è¯•å®‰è£…..."
        # å°è¯•å®‰è£…git-lfs (å¦‚æœå¯èƒ½)
        if command -v apt-get >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y git-lfs
            git lfs install
            git lfs pull
        elif command -v yum >/dev/null 2>&1; then
            sudo yum install -y git-lfs
            git lfs install
            git lfs pull
        else
            echo "âŒ æ— æ³•è‡ªåŠ¨å®‰è£…Git LFSï¼Œç»§ç»­å…¶ä»–æ–¹æ³•..."
        fi
    fi
else
    echo "âŒ Git cloneå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•..."
fi

# æ£€æŸ¥å…³é”®æ–‡ä»¶
echo ""
echo "ğŸ” æ£€æŸ¥å…³é”®æ–‡ä»¶çŠ¶æ€..."

check_file_integrity() {
    local file=$1
    local min_size=$2
    local file_desc=$3
    
    if [ -f "$file" ]; then
        local size=$(stat -c%s "$file" 2>/dev/null || echo "0")
        echo "ğŸ“‹ $file_desc: $size bytes"
        
        if [ "$size" -gt "$min_size" ]; then
            echo "âœ… $file_desc å¤§å°æ­£å¸¸"
            return 0
        else
            echo "âŒ $file_desc å¤§å°å¼‚å¸¸ (å¯èƒ½æ˜¯LFSæŒ‡é’ˆæ–‡ä»¶)"
            return 1
        fi
    else
        echo "âŒ $file_desc ä¸å­˜åœ¨"
        return 1
    fi
}

# æ£€æŸ¥tokenizer.model
TOKENIZER_OK=false
if check_file_integrity "tokenizer.model" 1000000 "tokenizer.model"; then
    TOKENIZER_OK=true
fi

# æ£€æŸ¥æƒé‡æ–‡ä»¶
WEIGHTS_OK=false
if check_file_integrity "pytorch_model-00001-of-00007.bin" 100000000 "æƒé‡æ–‡ä»¶"; then
    WEIGHTS_OK=true
fi

# å¦‚æœæ–‡ä»¶ä¸å®Œæ•´ï¼Œå°è¯•Pythonæ–¹æ³•ä¸‹è½½
if [ "$TOKENIZER_OK" = false ] || [ "$WEIGHTS_OK" = false ]; then
    echo ""
    echo "ğŸ”„ æ–¹æ³•2: ä½¿ç”¨Python huggingface_hubä¸‹è½½..."
    
    python3 -c "
import os
import sys
from pathlib import Path

print('ğŸ Pythonä¸‹è½½æ–¹æ³•å¯åŠ¨...')

try:
    from huggingface_hub import snapshot_download
    print('âœ… huggingface_hubå¯ç”¨')
    
    # ä¸‹è½½å®Œæ•´æ¨¡å‹
    print('ğŸ“¥ ä¸‹è½½å®Œæ•´ChatGLM3æ¨¡å‹...')
    local_dir = Path('.')
    
    snapshot_download(
        repo_id='THUDM/chatglm3-6b',
        local_dir=local_dir,
        local_dir_use_symlinks=False,
        resume_download=True,
        force_download=False
    )
    
    print('âœ… Pythonä¸‹è½½å®Œæˆ')
    
except ImportError:
    print('âŒ huggingface_hubæœªå®‰è£…ï¼Œå°è¯•å®‰è£…...')
    import subprocess
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'huggingface_hub>=0.16.0'])
        print('âœ… huggingface_hubå®‰è£…æˆåŠŸï¼Œé‡æ–°å°è¯•ä¸‹è½½...')
        
        from huggingface_hub import snapshot_download
        snapshot_download(
            repo_id='THUDM/chatglm3-6b',
            local_dir=Path('.'),
            local_dir_use_symlinks=False,
            resume_download=True
        )
        print('âœ… Pythonä¸‹è½½å®Œæˆ')
    except Exception as e:
        print(f'âŒ å®‰è£…å¤±è´¥: {e}')
        sys.exit(1)
        
except Exception as e:
    print(f'âŒ Pythonä¸‹è½½å¤±è´¥: {e}')
    sys.exit(1)
"
fi

# å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•wgetç›´æ¥ä¸‹è½½å…³é”®æ–‡ä»¶
echo ""
echo "ğŸ”„ æ–¹æ³•3: ç›´æ¥ä¸‹è½½å…³é”®æ–‡ä»¶..."

download_critical_files() {
    echo "ğŸ“¥ ä¸‹è½½å…³é”®æ–‡ä»¶..."
    
    # æ–‡ä»¶URLæ˜ å°„
    declare -A FILE_URLS=(
        ["config.json"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/config.json"
        ["tokenizer_config.json"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer_config.json"
        ["special_tokens_map.json"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/special_tokens_map.json"
        ["tokenizer.model"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer.model"
        ["modeling_chatglm.py"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/modeling_chatglm.py"
        ["tokenization_chatglm.py"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenization_chatglm.py"
        ["configuration_chatglm.py"]="https://huggingface.co/THUDM/chatglm3-6b/resolve/main/configuration_chatglm.py"
    )
    
    for file in "${!FILE_URLS[@]}"; do
        url="${FILE_URLS[$file]}"
        echo "  ğŸ“„ ä¸‹è½½ $file..."
        
        if wget -q --timeout=60 "$url" -O "$file"; then
            local size=$(stat -c%s "$file" 2>/dev/null || echo "0")
            echo "    âœ… $file ä¸‹è½½æˆåŠŸ ($size bytes)"
        else
            echo "    âš ï¸  $file wgetå¤±è´¥ï¼Œå°è¯•curl..."
            if curl -s --connect-timeout 60 "$url" -o "$file"; then
                local size=$(stat -c%s "$file" 2>/dev/null || echo "0")
                echo "    âœ… $file curlä¸‹è½½æˆåŠŸ ($size bytes)"
            else
                echo "    âŒ $file ä¸‹è½½å¤±è´¥"
            fi
        fi
    done
}

# æ£€æŸ¥tokenizeræ˜¯å¦è¿˜æœ‰é—®é¢˜
if [ ! -f "tokenizer.model" ] || [ "$(stat -c%s "tokenizer.model")" -lt 1000000 ]; then
    download_critical_files
fi

# æœ€ç»ˆéªŒè¯
echo ""
echo "ğŸ” æœ€ç»ˆéªŒè¯..."

# éªŒè¯tokenizeråŠŸèƒ½
echo "ğŸ”§ éªŒè¯tokenizeråŠŸèƒ½..."
python3 -c "
import sys
import os
sys.path.append('.')

try:
    import sentencepiece as spm
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists('tokenizer.model'):
        print('âŒ tokenizer.modelæ–‡ä»¶ä¸å­˜åœ¨')
        sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    size = os.path.getsize('tokenizer.model')
    print(f'ğŸ“‹ tokenizer.modelå¤§å°: {size} bytes')
    
    if size < 1000000:  # å°äº1MB
        print('âŒ tokenizer.modelæ–‡ä»¶è¿‡å°ï¼Œå¯èƒ½æŸå')
        sys.exit(1)
    
    # æµ‹è¯•åŠ è½½
    sp = smp.SentencePieceProcessor()
    sp.load('tokenizer.model')
    
    # æµ‹è¯•ç¼–ç 
    test_text = 'ä½ å¥½ï¼Œä¸–ç•Œï¼'
    tokens = sp.encode(test_text)
    decoded = sp.decode(tokens)
    
    print(f'âœ… tokenizeråŠŸèƒ½æ­£å¸¸')
    print(f'   æµ‹è¯•æ–‡æœ¬: {test_text}')
    print(f'   tokens: {tokens}')
    print(f'   è§£ç ç»“æœ: {decoded}')
    
except ImportError as e:
    print(f'âŒ sentencepieceæœªå®‰è£…: {e}')
    print('ğŸ’¡ å°è¯•å®‰è£…: pip install sentencepiece')
    sys.exit(1)
except Exception as e:
    print(f'âŒ tokenizeræµ‹è¯•å¤±è´¥: {e}')
    sys.exit(1)
" 2>/dev/null

if [ $? -eq 0 ]; then
    echo "ğŸ‰ tokenizeréªŒè¯é€šè¿‡!"
else
    echo "âŒ tokenizeréªŒè¯å¤±è´¥"
    
    # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä»å…¶ä»–æºä¸‹è½½
    echo ""
    echo "ğŸ”„ æœ€åå°è¯•ï¼šä»ModelScopeä¸‹è½½tokenizer..."
    
    python3 -c "
import urllib.request
import os

try:
    # ModelScopeé•œåƒURL
    url = 'https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenizer.model'
    
    print('ğŸ“¥ ä»ModelScopeä¸‹è½½tokenizer.model...')
    urllib.request.urlretrieve(url, 'tokenizer.model')
    
    size = os.path.getsize('tokenizer.model')
    print(f'âœ… ä¸‹è½½å®Œæˆï¼Œå¤§å°: {size} bytes')
    
except Exception as e:
    print(f'âŒ ModelScopeä¸‹è½½å¤±è´¥: {e}')
"
fi

# æ˜¾ç¤ºç›®å½•å†…å®¹
echo ""
echo "ğŸ“Š å½“å‰ç›®å½•å†…å®¹:"
ls -la

# æ£€æŸ¥å…³é”®æ–‡ä»¶
echo ""
echo "ğŸ” å…³é”®æ–‡ä»¶æ£€æŸ¥:"
for file in "config.json" "tokenizer.model" "modeling_chatglm.py" "tokenization_chatglm.py"; do
    if [ -f "$file" ]; then
        size=$(stat -c%s "$file")
        echo "âœ… $file: $size bytes"
    else
        echo "âŒ $file: ç¼ºå¤±"
    fi
done

echo ""
echo "ğŸ‰ ChatGLM3 å®Œæ•´ä¿®å¤å®Œæˆï¼"

# æœ€ç»ˆçš„tokenizeræµ‹è¯•
echo ""
echo "ğŸ§ª æœ€ç»ˆtokenizeræµ‹è¯•..."
python3 -c "
try:
    import sentencepiece as smp
    sp = smp.SentencePieceProcessor()
    sp.load('tokenizer.model')
    
    # ç¼–ç æµ‹è¯•
    text = 'ChatGLM3æ˜¯ä¸€ä¸ªå¯¹è¯è¯­è¨€æ¨¡å‹'
    tokens = sp.encode(text)
    decoded = sp.decode(tokens)
    
    print('ğŸ¯ Tokenizeræœ€ç»ˆæµ‹è¯•ç»“æœ:')
    print(f'   åŸæ–‡: {text}')
    print(f'   Tokenæ•°é‡: {len(tokens)}')
    print(f'   è§£ç ç»“æœ: {decoded}')
    print('âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ!')
    
except Exception as e:
    print(f'âŒ æœ€ç»ˆæµ‹è¯•å¤±è´¥: {e}')
    print('ğŸ”§ å»ºè®®æ‰‹åŠ¨æ£€æŸ¥tokenizer.modelæ–‡ä»¶')
    exit(1)
"

echo ""
echo "ğŸ“‹ ä¸‹ä¸€æ­¥ï¼š"
echo "cd $ONTOTHINK_ROOT"
echo "python3 enflame_training/scripts/train_ontothink_enflame.py --step full"
