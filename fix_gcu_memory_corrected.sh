#!/bin/bash

# ğŸ”§ ä¿®å¤GCUå†…å­˜åˆ†é…é—®é¢˜ (ä¿®æ­£ç‰ˆ)
# ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°ï¼Œä½¿ç”¨æ­£ç¡®çš„ç‡§åŸè„šæœ¬å‚æ•°
# ============================================

echo "ğŸ”§ ä¿®å¤GCUå†…å­˜åˆ†é…é—®é¢˜ (ä¿®æ­£ç‰ˆ)"
echo "ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°"
echo "========================="

echo ""
echo "ğŸ¯ é—®é¢˜åˆ†æï¼š"
echo "âœ… åˆ†å¸ƒå¼åˆå§‹åŒ–æˆåŠŸ"
echo "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
echo "âœ… DeepSpeedé…ç½®æ­£ç¡®"
echo "âŒ è„šæœ¬å‚æ•°é”™è¯¯: --save_interval, --checkpoint_interval ä¸æ”¯æŒ"

echo ""
echo "ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„ç‡§åŸè„šæœ¬å‚æ•°"
echo "=========================="

# ç‡§åŸè„šæœ¬ç›®å½•
ENFLAME_SCRIPT_DIR="/workspace/code/OntoThink_V4/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"

if [ ! -f "$ENFLAME_SCRIPT_DIR/finetune_chatglm3_for_multiturn.py" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨"
    exit 1
fi

echo "âœ… æ‰¾åˆ°è®­ç»ƒè„šæœ¬: $ENFLAME_SCRIPT_DIR"
cd "$ENFLAME_SCRIPT_DIR"

# è®¾ç½®ç‡§åŸåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=2
export ECCL_MAX_NCHANNELS=1
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"

# åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼ˆå•å¡é…ç½®ï¼‰
export MASTER_ADDR=localhost
export MASTER_PORT=29500
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export GCU_VISIBLE_DEVICES=0

# ç‡§åŸç‰¹å®šçš„åˆ†å¸ƒå¼å˜é‡
export CUDA_VISIBLE_DEVICES=""
export PTEX_DDP_BACKEND=eccl

echo "ğŸ¯ å†…å­˜ä¼˜åŒ–å‚æ•°ï¼š"
echo "  - micro_batch_size: 1 (æœ€å°)"
echo "  - gradient_accumulation: 2 (å¤§å¹…å‡å°‘)"
echo "  - max_tokens: 256 (å‡å°‘åºåˆ—é•¿åº¦)"
echo "  - max_steps: 3 (å¿«é€Ÿæµ‹è¯•)"
echo "  - train_epochs: 1 (æœ€å°è½®æ•°)"

echo ""
echo "ğŸš€ å¯åŠ¨ä¿®æ­£çš„å†…å­˜ä¼˜åŒ–å•GCUè®­ç»ƒ"
echo "============================="

# å¯åŠ¨è®­ç»ƒï¼Œåªä½¿ç”¨æ”¯æŒçš„å‚æ•°
python3.8 finetune_chatglm3_for_multiturn.py \
    --model_path "/workspace/code/OntoThink_V4/enflame_training/models/THUDM/chatglm3-6b" \
    --train_file "/workspace/code/OntoThink_V4/enflame_training/datasets/ontothink_multiturn/train.jsonl" \
    --tp_size 1 \
    --dp_size 1 \
    --pp_size 1 \
    --train_micro_batch_size 1 \
    --gradient_accumulation_steps 2 \
    --max_steps 3 \
    --max_tokens 256 \
    --ladder_shape False \
    --skip_steps 1 \
    --eval_batch_size 1 \
    --eval_per_n_epochs 1 \
    --train_epochs 1 2>&1 | tee /tmp/memory_optimized_corrected.log

echo ""
echo "ğŸ” ä¿®æ­£åçš„ç»“æœåˆ†æ"
echo "=================="

if [ -f /tmp/memory_optimized_corrected.log ]; then
    echo "ğŸ“‹ æ£€æŸ¥æœ€æ–°çš„è¾“å‡º:"
    tail -20 /tmp/memory_optimized_corrected.log
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥å‚æ•°é”™è¯¯ï¼š"
    if grep -q -i "unrecognized arguments\|error.*argument" /tmp/memory_optimized_corrected.log; then
        echo "âŒ ä»æœ‰å‚æ•°é”™è¯¯:"
        grep -i "unrecognized arguments\|error.*argument" /tmp/memory_optimized_corrected.log
    else
        echo "âœ… å‚æ•°é”™è¯¯å·²è§£å†³"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥ECCLåˆå§‹åŒ–ï¼š"
    if grep -q -i "ecclCommInitRank.*success" /tmp/memory_optimized_corrected.log; then
        echo "âœ… ECCLåˆå§‹åŒ–æˆåŠŸ"
    else
        echo "âš ï¸  ç­‰å¾…ECCLåˆå§‹åŒ–"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥DeepSpeedï¼š"
    if grep -q -i "DeepSpeed.*Optimizer\|Creating.*optimizer\|DeepSpeed.*Basic" /tmp/memory_optimized_corrected.log; then
        echo "âœ… DeepSpeedæ­£å¸¸å·¥ä½œ:"
        grep -i "DeepSpeed.*Optimizer\|Creating.*optimizer\|DeepSpeed.*Basic" /tmp/memory_optimized_corrected.log | tail -2
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥å†…å­˜åˆ†é…ï¼š"
    if grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess\|memory.*error" /tmp/memory_optimized_corrected.log; then
        echo "âŒ ä»æœ‰å†…å­˜åˆ†é…é—®é¢˜:"
        grep -i -A2 -B2 "topsMalloc.*failed\|Check failed.*topsSuccess\|memory.*error" /tmp/memory_optimized_corrected.log | tail -5
    else
        echo "âœ… å†…å­˜åˆ†é…æ­£å¸¸"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥è®­ç»ƒæ­¥éª¤ï¼š"
    if grep -q -i "step.*loss\|epoch.*step\|training.*step\|global.*step\|loss.*:" /tmp/memory_optimized_corrected.log; then
        echo "ğŸ‰ æ‰¾åˆ°è®­ç»ƒæ­¥éª¤:"
        grep -i "step.*loss\|epoch.*step\|training.*step\|global.*step\|loss.*:" /tmp/memory_optimized_corrected.log | tail -3
    else
        echo "âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ­¥éª¤"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼š"
    if grep -q -i "error\|fail\|exception\|abort\|traceback" /tmp/memory_optimized_corrected.log; then
        echo "âŒ å‘ç°é”™è¯¯:"
        grep -i -A1 -B1 "error\|fail\|exception\|abort\|traceback" /tmp/memory_optimized_corrected.log | tail -6
    else
        echo "âœ… æ²¡æœ‰å‘ç°é”™è¯¯"
    fi
fi

echo ""
echo "ğŸ’¡ ä¿®æ­£åçš„ç»“æœæ€»ç»“"
echo "=================="

if [ -f /tmp/memory_optimized_corrected.log ]; then
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
    if grep -q -i "step.*loss\|epoch.*step\|loss.*:" /tmp/memory_optimized_corrected.log && ! grep -q -i "unrecognized arguments\|topsMalloc.*failed" /tmp/memory_optimized_corrected.log; then
        echo "ğŸ‰ è®­ç»ƒæˆåŠŸå¯åŠ¨ï¼"
        echo "âœ… å‚æ•°æ­£ç¡®"
        echo "âœ… å†…å­˜åˆ†é…æ­£å¸¸"
        echo "âœ… è®­ç»ƒæ­¥éª¤å¼€å§‹"
        echo ""
        echo "ğŸš€ ä¸‹ä¸€æ­¥ï¼šæ‰©å±•è®­ç»ƒé…ç½®"
        echo "1. å¢åŠ åºåˆ—é•¿åº¦å’Œbatch size"
        echo "2. å¢åŠ è®­ç»ƒæ­¥æ•°"
        echo "3. å°è¯•å¤šå¡å¹¶è¡Œè®­ç»ƒ"
    elif grep -q -i "DeepSpeed.*Optimizer\|Creating.*optimizer" /tmp/memory_optimized_corrected.log && ! grep -q -i "unrecognized arguments\|topsMalloc.*failed" /tmp/memory_optimized_corrected.log; then
        echo "ğŸ¯ ä¼˜åŒ–å™¨æˆåŠŸï¼Œæ£€æŸ¥è®­ç»ƒå¾ªç¯"
        echo "âœ… å‚æ•°æ­£ç¡®"
        echo "âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ"
        echo "âš ï¸  å¯èƒ½åœ¨è®­ç»ƒæ•°æ®å¤„ç†ä¸­"
    elif ! grep -q -i "unrecognized arguments" /tmp/memory_optimized_corrected.log; then
        echo "âœ… å‚æ•°é—®é¢˜å·²è§£å†³"
        echo "ğŸ”§ ç»§ç»­è°ƒè¯•å…¶ä»–é—®é¢˜"
    else
        echo "âš ï¸  ä»æœ‰å…¶ä»–é—®é¢˜éœ€è¦è§£å†³"
    fi
fi

echo ""
echo "ğŸ“‹ å®Œæ•´æ—¥å¿—æ–‡ä»¶: /tmp/memory_optimized_corrected.log"
echo "ğŸ“‹ å½“å‰ä¼˜åŒ–é…ç½®:"
echo "  GCU_VISIBLE_DEVICES=$GCU_VISIBLE_DEVICES"
echo "  WORLD_SIZE=$WORLD_SIZE"
echo "  max_tokens=256, micro_batch=1, grad_accum=2, max_steps=3"
