#!/bin/bash

# ğŸ”§ ä¿®å¤GCUå†…å­˜åˆ†é…é—®é¢˜
# é€šè¿‡å‡å°‘æ¨¡å‹å¤§å°å’Œä¼˜åŒ–å™¨çŠ¶æ€æ¥è§£å†³å†…å­˜ä¸è¶³
# =============================================

echo "ğŸ”§ ä¿®å¤GCUå†…å­˜åˆ†é…é—®é¢˜"
echo "é€šè¿‡å‡å°‘æ¨¡å‹å¤§å°å’Œä¼˜åŒ–å™¨çŠ¶æ€"
echo "==========================="

echo ""
echo "ğŸ¯ é—®é¢˜åˆ†æï¼š"
echo "âœ… åˆ†å¸ƒå¼åˆå§‹åŒ–æˆåŠŸ"
echo "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ"
echo "âœ… DeepSpeedé…ç½®æ­£ç¡®"
echo "âŒ GCUå†…å­˜ä¸è¶³: topsMalloc failed"
echo "ğŸ’¡ é—®é¢˜å‡ºç°åœ¨ä¼˜åŒ–å™¨çŠ¶æ€åˆå§‹åŒ–é˜¶æ®µ"

echo ""
echo "ğŸ”§ å†…å­˜ä¼˜åŒ–ç­–ç•¥"
echo "================"

# ç‡§åŸè„šæœ¬ç›®å½•
ENFLAME_SCRIPT_DIR="/workspace/code/OntoThink_V4/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"

if [ ! -f "$ENFLAME_SCRIPT_DIR/finetune_chatglm3_for_multiturn.py" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨"
    exit 1
fi

echo "âœ… æ‰¾åˆ°è®­ç»ƒè„šæœ¬: $ENFLAME_SCRIPT_DIR"
cd "$ENFLAME_SCRIPT_DIR"

# è®¾ç½®ç‡§åŸåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼ˆä»ä¹‹å‰ä¿®å¤çš„é…ç½®ï¼‰
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=2  # å‡å°‘çº¿ç¨‹æ•°
export ECCL_MAX_NCHANNELS=1
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"

# åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼ˆå•å¡é…ç½®ï¼‰
export MASTER_ADDR=localhost
export MASTER_PORT=29500
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export GCU_VISIBLE_DEVICES=0

# ç‡§åŸç‰¹å®šçš„åˆ†å¸ƒå¼å˜é‡
export CUDA_VISIBLE_DEVICES=""
export PTEX_DDP_BACKEND=eccl

echo "ğŸ¯ å†…å­˜ä¼˜åŒ–å‚æ•°ï¼š"
echo "  - micro_batch_size: 1 â†’ 1 (ä¿æŒæœ€å°)"
echo "  - gradient_accumulation: 64 â†’ 2 (å¤§å¹…å‡å°‘)"
echo "  - max_tokens: 2048 â†’ 256 (å‡å°‘åºåˆ—é•¿åº¦)"
echo "  - max_steps: âˆ â†’ 3 (å¿«é€Ÿæµ‹è¯•)"
echo "  - fp16: enabled (å‡å°‘å†…å­˜ä½¿ç”¨)"
echo "  - OMP_NUM_THREADS: 5 â†’ 2 (å‡å°‘çº¿ç¨‹)"

echo ""
echo "ğŸš€ å¯åŠ¨å†…å­˜ä¼˜åŒ–çš„å•GCUè®­ç»ƒ"
echo "============================"

# å¯åŠ¨è¶…å°å†…å­˜å•å¡è®­ç»ƒ
python3.8 finetune_chatglm3_for_multiturn.py \
    --model_path "/workspace/code/OntoThink_V4/enflame_training/models/THUDM/chatglm3-6b" \
    --train_file "/workspace/code/OntoThink_V4/enflame_training/datasets/ontothink_multiturn/train.jsonl" \
    --tp_size 1 \
    --dp_size 1 \
    --pp_size 1 \
    --train_micro_batch_size 1 \
    --gradient_accumulation_steps 2 \
    --max_steps 3 \
    --max_tokens 256 \
    --ladder_shape False \
    --skip_steps 1 \
    --eval_batch_size 1 \
    --eval_per_n_epochs 1 \
    --train_epochs 1 \
    --save_interval 999 \
    --checkpoint_interval 999 2>&1 | tee /tmp/memory_optimized_gcu.log

echo ""
echo "ğŸ” å†…å­˜ä¼˜åŒ–ç»“æœåˆ†æ"
echo "===================="

if [ -f /tmp/memory_optimized_gcu.log ]; then
    echo "ğŸ“‹ æ£€æŸ¥æœ€æ–°çš„è¾“å‡º:"
    tail -20 /tmp/memory_optimized_gcu.log
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥å†…å­˜åˆ†é…ï¼š"
    if grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess\|memory.*error" /tmp/memory_optimized_gcu.log; then
        echo "âŒ ä»æœ‰å†…å­˜åˆ†é…é—®é¢˜:"
        grep -i -A2 -B2 "topsMalloc.*failed\|Check failed.*topsSuccess\|memory.*error" /tmp/memory_optimized_gcu.log | tail -5
    else
        echo "âœ… å†…å­˜åˆ†é…é—®é¢˜å·²è§£å†³"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥ä¼˜åŒ–å™¨åˆå§‹åŒ–ï¼š"
    if grep -q -i "optimizer.*init\|DeepSpeed.*Basic.*Optimizer\|Creating.*optimizer" /tmp/memory_optimized_gcu.log; then
        echo "âœ… æ‰¾åˆ°ä¼˜åŒ–å™¨åˆå§‹åŒ–:"
        grep -i "optimizer.*init\|DeepSpeed.*Basic.*Optimizer\|Creating.*optimizer" /tmp/memory_optimized_gcu.log | tail -3
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥è®­ç»ƒæ­¥éª¤ï¼š"
    if grep -q -i "step.*loss\|epoch.*\|training.*step\|global.*step" /tmp/memory_optimized_gcu.log; then
        echo "ğŸ‰ æ‰¾åˆ°è®­ç»ƒæ­¥éª¤:"
        grep -i "step.*loss\|epoch.*\|training.*step\|global.*step" /tmp/memory_optimized_gcu.log | tail -5
    else
        echo "âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ­¥éª¤"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥ECCLçŠ¶æ€ï¼š"
    if grep -q -i "ecclCommInitRank.*success" /tmp/memory_optimized_gcu.log; then
        echo "âœ… ECCLæ­£å¸¸å·¥ä½œ"
    else
        echo "âš ï¸  ECCLçŠ¶æ€æœªçŸ¥"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼š"
    if grep -q -i "error\|fail\|exception\|abort" /tmp/memory_optimized_gcu.log; then
        echo "âŒ å‘ç°é”™è¯¯:"
        grep -i -A1 -B1 "error\|fail\|exception\|abort" /tmp/memory_optimized_gcu.log | tail -6
    else
        echo "âœ… æ²¡æœ‰å‘ç°é”™è¯¯"
    fi
fi

echo ""
echo "ğŸ’¡ å†…å­˜ä¼˜åŒ–ç»“æœæ€»ç»“"
echo "==================="

if [ -f /tmp/memory_optimized_gcu.log ]; then
    # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
    if grep -q -i "step.*loss\|training.*completed\|epoch.*step" /tmp/memory_optimized_gcu.log && ! grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess" /tmp/memory_optimized_gcu.log; then
        echo "ğŸ‰ å†…å­˜ä¼˜åŒ–æˆåŠŸï¼"
        echo "âœ… GCUå†…å­˜åˆ†é…æ­£å¸¸"
        echo "âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ"
        echo "âœ… è®­ç»ƒæ­¥éª¤æ­£å¸¸"
        echo ""
        echo "ğŸš€ ä¸‹ä¸€æ­¥ï¼šé€æ­¥æ‰©å±•é…ç½®"
        echo "1. å¢åŠ åºåˆ—é•¿åº¦: 256 â†’ 512 â†’ 1024"
        echo "2. å¢åŠ batch size: 1 â†’ 2 â†’ 4"
        echo "3. å¢åŠ å¹¶è¡Œåº¦: 1å¡ â†’ 2å¡ â†’ 4å¡ â†’ 8å¡"
    elif grep -q -i "DeepSpeed.*Optimizer\|Creating.*optimizer" /tmp/memory_optimized_gcu.log && ! grep -q -i "topsMalloc.*failed" /tmp/memory_optimized_gcu.log; then
        echo "ğŸ¯ éƒ¨åˆ†æˆåŠŸï¼"
        echo "âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ"
        echo "âš ï¸  å¯èƒ½åœ¨è®­ç»ƒå¾ªç¯ä¸­é‡åˆ°å…¶ä»–é—®é¢˜"
        echo "ğŸ’¡ å»ºè®®è¿›ä¸€æ­¥è°ƒè¯•è®­ç»ƒå¾ªç¯"
    else
        echo "âš ï¸  ä»éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"
        echo "ğŸ“‹ å®Œæ•´æ—¥å¿—: /tmp/memory_optimized_gcu.log"
        echo ""
        echo "ğŸ”§ æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–å»ºè®®ï¼š"
        echo "1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–checkpoint"
        echo "2. ç¦ç”¨ä¸€äº›DeepSpeedåŠŸèƒ½"
        echo "3. å‡å°‘æ›´å¤šå‚æ•°"
    fi
fi

echo ""
echo "ğŸ“‹ å½“å‰ä¼˜åŒ–é…ç½®ï¼š"
echo "GCU_VISIBLE_DEVICES=$GCU_VISIBLE_DEVICES"
echo "WORLD_SIZE=$WORLD_SIZE"
echo "OMP_NUM_THREADS=$OMP_NUM_THREADS"
echo "max_tokens=256, micro_batch=1, grad_accum=2"
