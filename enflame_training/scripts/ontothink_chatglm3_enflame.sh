#!/bin/bash
#
# OntoThink ChatGLM3-6B ç‡§åŽŸT20è®­ç»ƒè„šæœ¬
# åŸºäºŽç‡§åŽŸå®˜æ–¹ChatGLM3å¾®è°ƒè„šæœ¬ä¼˜åŒ–
#
# Copyright (c) 2024 OntoThink Project. All rights reserved.
#

set -eu -o pipefail

# ============================== ç‡§åŽŸT20çŽ¯å¢ƒé…ç½® ================================
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=5
export ECCL_MAX_NCHANNELS=2
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"

# ============================== OntoThinkè®­ç»ƒé…ç½® ================================
# åŸºç¡€è·¯å¾„ - è‡ªåŠ¨æ£€æµ‹
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ENFLAME_ROOT="$(dirname "$SCRIPT_DIR")"
ONTOTHINK_ROOT="$(dirname "$ENFLAME_ROOT")"

echo "ðŸ“ é¡¹ç›®æ ¹ç›®å½•: $ONTOTHINK_ROOT"
echo "ðŸ“ ç‡§åŽŸç›®å½•: $ENFLAME_ROOT"

# æ¨¡åž‹é…ç½®
export PRETRAINED_MODEL_PATH=${PRETRAINED_MODEL_PATH:-"${ENFLAME_ROOT}/models/THUDM/chatglm3-6b"}
export TRAIN_FILE=${TRAIN_FILE:-"${ENFLAME_ROOT}/datasets/ontothink_multiturn/train.jsonl"}

# è®­ç»ƒå‚æ•°é…ç½®
export MAX_TOKENS=${MAX_TOKENS:-"2048"}  # OntoThinkéœ€è¦è¾ƒé•¿ä¸Šä¸‹æ–‡
export TP_SIZE=${TP_SIZE:-"1"}           # å¼ é‡å¹¶è¡Œå¤§å°
export DP_SIZE=${DP_SIZE:-"1"}           # æ•°æ®å¹¶è¡Œå¤§å°  
export PP_SIZE=${PP_SIZE:-"8"}           # æµæ°´çº¿å¹¶è¡Œå¤§å°(8å¡)
export LADDER_SHAPE=${LADDER_SHAPE:-"False"}
export SKIP_STEPS=${SKIP_STEPS:-"50"}    # è·³è¿‡æ­¥æ•°(å‡å°‘æ—¥å¿—)
export MAX_STEPS=${MAX_STEPS:-"-1"}      # æœ€å¤§è®­ç»ƒæ­¥æ•°(-1è¡¨ç¤ºæŒ‰epoch)
export MICRO_BATCH_SIZE=${MICRO_BATCH_SIZE:-"1"}           # å¾®æ‰¹æ¬¡å¤§å°
export GRADIENT_ACCUMULATION_STEPS=${GRADIENT_ACCUMULATION_STEPS:-"64"}  # æ¢¯åº¦ç´¯ç§¯
export EVAL_BATCH_SIZE=${EVAL_BATCH_SIZE:-"1"}             # è¯„ä¼°æ‰¹æ¬¡å¤§å°
export EVAL_PER_N_EPOCHS=${EVAL_PER_N_EPOCHS:-"1"}         # æ¯å‡ ä¸ªepochè¯„ä¼°ä¸€æ¬¡
export TRAIN_EPOCHS=${TRAIN_EPOCHS:-"3"}                   # è®­ç»ƒè½®æ•°

# è¾“å‡ºç›®å½•
export OUTPUT_DIR=${OUTPUT_DIR:-"${ENFLAME_ROOT}/models/ontothink-chatglm3-6b"}
export LOG_DIR=${LOG_DIR:-"${ENFLAME_ROOT}/logs"}

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p $OUTPUT_DIR
mkdir -p $LOG_DIR

# ============================== è®­ç»ƒä¿¡æ¯æ˜¾ç¤º ================================
echo "ðŸš€ OntoThink ChatGLM3-6B ç‡§åŽŸT20è®­ç»ƒ"
echo "==============================================="
echo "ðŸ“‹ è®­ç»ƒé…ç½®:"
echo "   - åŸºç¡€æ¨¡åž‹: $PRETRAINED_MODEL_PATH"
echo "   - è®­ç»ƒæ•°æ®: $TRAIN_FILE"
echo "   - æœ€å¤§åºåˆ—é•¿åº¦: $MAX_TOKENS"
echo "   - å¹¶è¡Œé…ç½®: TP=$TP_SIZE, DP=$DP_SIZE, PP=$PP_SIZE"
echo "   - æ‰¹æ¬¡é…ç½®: micro_batch=$MICRO_BATCH_SIZE, grad_accum=$GRADIENT_ACCUMULATION_STEPS"
echo "   - è®­ç»ƒè½®æ•°: $TRAIN_EPOCHS"
echo "   - è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "==============================================="

# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
if [ ! -d "$PRETRAINED_MODEL_PATH" ]; then
    echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°ChatGLM3-6Bæ¨¡åž‹ï¼Œè¯·å…ˆä¸‹è½½åˆ°: $PRETRAINED_MODEL_PATH"
    echo "ðŸ’¡ ä¸‹è½½å‘½ä»¤:"
    echo "   cd ${ENFLAME_ROOT}/models"
    echo "   git clone https://huggingface.co/THUDM/chatglm3-6b"
    exit 1
fi

# æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨
if [ ! -f "$TRAIN_FILE" ]; then
    echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬"
    echo "ðŸ’¡ å‡†å¤‡æ•°æ®å‘½ä»¤:"
    echo "   python ${ENFLAME_ROOT}/scripts/prepare_enflame_data.py \\"
    echo "     --input_dir ${ONTOTHINK_ROOT}/backend/data/processed \\"
    echo "     --output_dir ${ENFLAME_ROOT}/datasets/ontothink_multiturn \\"
    echo "     --format multiturn"
    exit 1
fi

# ============================== å¯åŠ¨è®­ç»ƒ ================================
echo "ðŸ”¥ å¯åŠ¨OntoThink ChatGLM3è®­ç»ƒ..."

# åˆ‡æ¢åˆ°ç‡§åŽŸè„šæœ¬ç›®å½•
cd ${ENFLAME_ROOT}/llm_scripts/finetuning/chatglm3

# å¤‡ä»½åŽŸå§‹è®­ç»ƒè„šæœ¬
if [ ! -f "finetune_chatglm3_for_multiturn_original.py" ]; then
    cp finetune_chatglm3_for_multiturn.py finetune_chatglm3_for_multiturn_original.py
fi

# æ£€æµ‹Pythonå‘½ä»¤
PYTHON_CMD="python3"
if command -v python3.8 &> /dev/null; then
    PYTHON_CMD="python3.8"
fi

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
LOG_FILE="${LOG_DIR}/ontothink_training_$(date +%Y%m%d_%H%M%S).log"

$PYTHON_CMD -u -m torch.distributed.launch \
    --nproc_per_node=8 \
    --standalone \
    --use_env finetune_chatglm3_for_multiturn.py \
    --model_path $PRETRAINED_MODEL_PATH \
    --train_file $TRAIN_FILE \
    --tp_size $TP_SIZE \
    --dp_size $DP_SIZE \
    --pp_size $PP_SIZE \
    --train_micro_batch_size $MICRO_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --max_steps $MAX_STEPS \
    --max_tokens $MAX_TOKENS \
    --ladder_shape $LADDER_SHAPE \
    --skip_steps $SKIP_STEPS \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --eval_per_n_epochs $EVAL_PER_N_EPOCHS \
    --train_epochs $TRAIN_EPOCHS \
    2>&1 | tee "$LOG_FILE"

# ============================== è®­ç»ƒå®Œæˆå¤„ç† ================================
if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo "âœ… OntoThink ChatGLM3è®­ç»ƒå®Œæˆï¼"
    echo "ðŸ“ æ¨¡åž‹ä¿å­˜ä½ç½®: $OUTPUT_DIR"
    echo "ðŸ“Š è®­ç»ƒæ—¥å¿—: $LOG_FILE"
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    cat > "${OUTPUT_DIR}/training_config.json" << EOF
{
    "model_name": "OntoThink-ChatGLM3-6B",
    "base_model": "$PRETRAINED_MODEL_PATH",
    "training_data": "$TRAIN_FILE",
    "max_tokens": $MAX_TOKENS,
    "parallel_config": {
        "tp_size": $TP_SIZE,
        "dp_size": $DP_SIZE,
        "pp_size": $PP_SIZE
    },
    "batch_config": {
        "micro_batch_size": $MICRO_BATCH_SIZE,
        "gradient_accumulation_steps": $GRADIENT_ACCUMULATION_STEPS
    },
    "training_epochs": $TRAIN_EPOCHS,
    "training_date": "$(date)",
    "hardware": "Enflame T20 8-cards"
}
EOF
    
    echo "ðŸ’¾ è®­ç»ƒé…ç½®å·²ä¿å­˜: ${OUTPUT_DIR}/training_config.json"
    
    # è¿è¡Œç®€å•éªŒè¯
    echo "ðŸ” å¼€å§‹æ¨¡åž‹éªŒè¯..."
    $PYTHON_CMD ${ENFLAME_ROOT}/scripts/validate_enflame_model.py \
        --model_path $OUTPUT_DIR \
        --output_path "${OUTPUT_DIR}/validation_results.json"
else
    echo "âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: $LOG_FILE"
    exit 1
fi

echo "ðŸŽ‰ OntoThink ç‡§åŽŸT20è®­ç»ƒæµç¨‹å®Œæˆï¼"
