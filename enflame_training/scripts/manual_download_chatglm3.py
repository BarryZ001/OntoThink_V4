#!/usr/bin/env python3
"""
ChatGLM3 æ‰‹åŠ¨ä¸‹è½½è„šæœ¬ - å½»åº•è§£å†³ä¸‹è½½é—®é¢˜
é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ
"""

import os
import sys
import hashlib
import urllib.request
import urllib.error
from pathlib import Path
import json
import time

def print_status(message, status="info"):
    """æ‰“å°å¸¦çŠ¶æ€çš„æ¶ˆæ¯"""
    symbols = {
        "info": "ğŸ“‹",
        "success": "âœ…", 
        "error": "âŒ",
        "warning": "âš ï¸",
        "progress": "ğŸ”„"
    }
    print(f"{symbols.get(status, 'ğŸ“‹')} {message}")

def download_file_with_progress(url, filepath, timeout=300):
    """ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦"""
    try:
        print_status(f"ä¸‹è½½: {os.path.basename(filepath)}", "progress")
        
        # åˆ›å»ºè¯·æ±‚
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
        
        with urllib.request.urlopen(req, timeout=timeout) as response:
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f:
                downloaded = 0
                chunk_size = 8192
                
                while True:
                    chunk = response.read(chunk_size)
                    if not chunk:
                        break
                    
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\r  è¿›åº¦: {percent:.1f}% ({downloaded}/{total_size} bytes)", end="", flush=True)
                
                print()  # æ¢è¡Œ
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        actual_size = os.path.getsize(filepath)
        if total_size > 0 and actual_size != total_size:
            print_status(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›{total_size}, å®é™…{actual_size}", "warning")
        else:
            print_status(f"ä¸‹è½½å®Œæˆ: {actual_size} bytes", "success")
        
        return True
        
    except Exception as e:
        print_status(f"ä¸‹è½½å¤±è´¥: {e}", "error")
        return False

def verify_file_integrity(filepath, min_size=None, expected_size=None):
    """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"""
    if not os.path.exists(filepath):
        print_status(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}", "error")
        return False
    
    size = os.path.getsize(filepath)
    
    if min_size and size < min_size:
        print_status(f"æ–‡ä»¶è¿‡å°: {size} < {min_size}", "error")
        return False
    
    if expected_size and size != expected_size:
        print_status(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: {size} != {expected_size}", "warning")
    
    print_status(f"æ–‡ä»¶å¤§å°æ­£å¸¸: {size} bytes", "success")
    return True

def test_tokenizer(model_dir):
    """æµ‹è¯•tokenizeråŠŸèƒ½"""
    tokenizer_path = os.path.join(model_dir, "tokenizer.model")
    
    try:
        import sentencepiece as spm
        
        print_status("æµ‹è¯•tokenizeråŠŸèƒ½...", "progress")
        
        # åŠ è½½tokenizer
        sp = spm.SentencePieceProcessor()
        sp.load(tokenizer_path)
        
        # æµ‹è¯•ç¼–ç è§£ç 
        test_texts = [
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "ChatGLM3 is a conversational language model.",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿã€‚",
            "What is the meaning of life?"
        ]
        
        for text in test_texts:
            tokens = sp.encode(text)
            decoded = sp.decode(tokens)
            
            if decoded.strip() != text.strip():
                print_status(f"ç¼–ç è§£ç ä¸ä¸€è‡´: '{text}' -> '{decoded}'", "warning")
            else:
                print_status(f"æµ‹è¯•é€šè¿‡: '{text}' ({len(tokens)} tokens)", "success")
        
        print_status("TokenizeråŠŸèƒ½éªŒè¯é€šè¿‡!", "success")
        return True
        
    except ImportError:
        print_status("sentencepieceæœªå®‰è£…", "error")
        print_status("è¯·å®‰è£…: pip install sentencepiece", "info")
        return False
    except Exception as e:
        print_status(f"Tokenizeræµ‹è¯•å¤±è´¥: {e}", "error")
        return False

def main():
    print("ğŸš€ ChatGLM3 æ‰‹åŠ¨ä¸‹è½½å™¨")
    print("é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ")
    print("=" * 50)
    
    # ç¡®å®šæ¨¡å‹ç›®å½•
    script_dir = Path(__file__).parent
    model_dir = script_dir.parent / "models" / "THUDM" / "chatglm3-6b"
    
    print_status(f"ç›®æ ‡ç›®å½•: {model_dir}", "info")
    
    # åˆ›å»ºç›®å½•
    model_dir.mkdir(parents=True, exist_ok=True)
    os.chdir(model_dir)
    
    print_status(f"å·¥ä½œç›®å½•: {os.getcwd()}", "info")
    
    # å®šä¹‰éœ€è¦ä¸‹è½½çš„æ–‡ä»¶
    files_to_download = {
        "config.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/config.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=config.json"
            ],
            "min_size": 100,
            "critical": True
        },
        "tokenizer_config.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer_config.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenizer_config.json"
            ],
            "min_size": 100,
            "critical": True
        },
        "special_tokens_map.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/special_tokens_map.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=special_tokens_map.json"
            ],
            "min_size": 10,
            "critical": True
        },
        "tokenizer.model": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer.model",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenizer.model"
            ],
            "min_size": 1000000,  # è‡³å°‘1MB
            "critical": True
        },
        "modeling_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/modeling_chatglm.py",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=modeling_chatglm.py"
            ],
            "min_size": 10000,
            "critical": True
        },
        "tokenization_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenization_chatglm.py", 
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenization_chatglm.py"
            ],
            "min_size": 5000,
            "critical": True
        },
        "configuration_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/configuration_chatglm.py",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=configuration_chatglm.py"
            ],
            "min_size": 1000,
            "critical": True
        }
    }
    
    # ä¸‹è½½æ–‡ä»¶
    success_count = 0
    total_files = len(files_to_download)
    
    for filename, file_info in files_to_download.items():
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        if verify_file_integrity(filename, file_info["min_size"]):
            print_status(f"{filename} å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡ä¸‹è½½", "success")
            success_count += 1
            continue
        
        # å°è¯•ä¸‹è½½
        downloaded = False
        for i, url in enumerate(file_info["urls"]):
            print_status(f"å°è¯•æº {i+1}/{len(file_info['urls'])}: {url.split('/')[-1]}", "progress")
            
            if download_file_with_progress(url, filename):
                if verify_file_integrity(filename, file_info["min_size"]):
                    downloaded = True
                    success_count += 1
                    break
                else:
                    print_status("ä¸‹è½½çš„æ–‡ä»¶æ— æ•ˆï¼Œåˆ é™¤å¹¶å°è¯•ä¸‹ä¸€ä¸ªæº", "warning")
                    try:
                        os.remove(filename)
                    except:
                        pass
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        if not downloaded:
            print_status(f"æ‰€æœ‰æºéƒ½å¤±è´¥: {filename}", "error")
            if file_info["critical"]:
                print_status("è¿™æ˜¯å…³é”®æ–‡ä»¶ï¼Œä¸‹è½½å¤±è´¥å¯èƒ½å½±å“è®­ç»ƒ", "error")
    
    # æ€»ç»“
    print(f"\n{'='*50}")
    print_status(f"ä¸‹è½½å®Œæˆ: {success_count}/{total_files} æ–‡ä»¶æˆåŠŸ", "info")
    
    if success_count == total_files:
        print_status("æ‰€æœ‰æ–‡ä»¶ä¸‹è½½æˆåŠŸ!", "success")
        
        # æµ‹è¯•tokenizer
        if test_tokenizer(model_dir):
            print_status("ChatGLM3æ¨¡å‹å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ!", "success")
            return 0
        else:
            print_status("Tokenizeræµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥", "error")
            return 1
    else:
        print_status("éƒ¨åˆ†æ–‡ä»¶ä¸‹è½½å¤±è´¥", "error")
        return 1

if __name__ == "__main__":
    sys.exit(main())
