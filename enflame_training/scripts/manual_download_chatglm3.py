#!/usr/bin/env python3
"""
ChatGLM3 æ‰‹åŠ¨ä¸‹è½½è„šæœ¬ - å½»åº•è§£å†³ä¸‹è½½é—®é¢˜
é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ
"""

import os
import sys
import hashlib
import urllib.request
import urllib.error
from pathlib import Path
import json
import time

def print_status(message, status="info"):
    """æ‰“å°å¸¦çŠ¶æ€çš„æ¶ˆæ¯"""
    symbols = {
        "info": "ğŸ“‹",
        "success": "âœ…", 
        "error": "âŒ",
        "warning": "âš ï¸",
        "progress": "ğŸ”„"
    }
    print(f"{symbols.get(status, 'ğŸ“‹')} {message}")

def download_file_with_progress(url, filepath, timeout=300):
    """ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦"""
    try:
        print_status(f"ä¸‹è½½: {os.path.basename(filepath)}", "progress")
        
        # åˆ›å»ºè¯·æ±‚
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36')
        
        with urllib.request.urlopen(req, timeout=timeout) as response:
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f:
                downloaded = 0
                chunk_size = 8192
                
                while True:
                    chunk = response.read(chunk_size)
                    if not chunk:
                        break
                    
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"\r  è¿›åº¦: {percent:.1f}% ({downloaded}/{total_size} bytes)", end="", flush=True)
                
                print()  # æ¢è¡Œ
        
        # éªŒè¯æ–‡ä»¶å¤§å°
        actual_size = os.path.getsize(filepath)
        if total_size > 0 and actual_size != total_size:
            print_status(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›{total_size}, å®é™…{actual_size}", "warning")
        else:
            print_status(f"ä¸‹è½½å®Œæˆ: {actual_size} bytes", "success")
        
        return True
        
    except Exception as e:
        print_status(f"ä¸‹è½½å¤±è´¥: {e}", "error")
        return False

def verify_file_integrity(filepath, min_size=None, expected_size=None):
    """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"""
    if not os.path.exists(filepath):
        print_status(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}", "error")
        return False
    
    size = os.path.getsize(filepath)
    
    if min_size and size < min_size:
        print_status(f"æ–‡ä»¶è¿‡å°: {size} < {min_size}", "error")
        return False
    
    if expected_size and size != expected_size:
        print_status(f"æ–‡ä»¶å¤§å°ä¸åŒ¹é…: {size} != {expected_size}", "warning")
    
    print_status(f"æ–‡ä»¶å¤§å°æ­£å¸¸: {size} bytes", "success")
    return True

def check_existing_model(model_dir):
    """æ£€æŸ¥ç°æœ‰æ¨¡å‹æ–‡ä»¶çš„å®Œæ•´æ€§"""
    print_status("æ£€æŸ¥ç°æœ‰æ¨¡å‹æ–‡ä»¶...", "progress")
    
    if not os.path.exists(model_dir):
        print_status("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨", "info")
        return {"need_download": True, "missing_files": []}
    
    os.chdir(model_dir)
    
    # æ£€æŸ¥æƒé‡æ–‡ä»¶
    weight_files_ok = 0
    weight_files_total = 7
    
    print_status("æ£€æŸ¥æƒé‡æ–‡ä»¶...", "progress") 
    for i in range(1, 8):
        safetensor_file = f"model-0000{i}-of-00007.safetensors"
        pytorch_file = f"pytorch_model-0000{i}-of-00007.bin"
        
        if os.path.exists(safetensor_file):
            size = os.path.getsize(safetensor_file)
            if size > 100_000_000:  # å¤§äº100MB
                print_status(f"æƒé‡æ–‡ä»¶ {i}: {safetensor_file} ({size:,} bytes)", "success")
                weight_files_ok += 1
            else:
                print_status(f"æƒé‡æ–‡ä»¶ {i}: {safetensor_file} ({size} bytes) - å¯èƒ½æ˜¯LFSæŒ‡é’ˆ", "warning")
        elif os.path.exists(pytorch_file):
            size = os.path.getsize(pytorch_file)
            if size > 100_000_000:  # å¤§äº100MB  
                print_status(f"æƒé‡æ–‡ä»¶ {i}: {pytorch_file} ({size:,} bytes)", "success")
                weight_files_ok += 1
            else:
                print_status(f"æƒé‡æ–‡ä»¶ {i}: {pytorch_file} ({size} bytes) - å¯èƒ½æ˜¯LFSæŒ‡é’ˆ", "warning")
        else:
            print_status(f"æƒé‡æ–‡ä»¶ {i}: ç¼ºå¤±", "error")
    
    weights_complete = weight_files_ok >= weight_files_total
    print_status(f"æƒé‡æ–‡ä»¶çŠ¶æ€: {weight_files_ok}/{weight_files_total} å®Œæ•´", 
                "success" if weights_complete else "warning")
    
    # æ£€æŸ¥tokenizer
    tokenizer_ok = False
    if os.path.exists("tokenizer.model"):
        size = os.path.getsize("tokenizer.model")
        if size > 1_000_000:  # å¤§äº1MB
            print_status(f"tokenizer.model: {size:,} bytes", "success")
            tokenizer_ok = True
        else:
            print_status(f"tokenizer.model: {size} bytes - è¿‡å°", "warning")
    else:
        print_status("tokenizer.model: ç¼ºå¤±", "error")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_files = [
        "config.json", "tokenizer_config.json", "special_tokens_map.json",
        "modeling_chatglm.py", "tokenization_chatglm.py", "configuration_chatglm.py"
    ]
    
    missing_config = []
    for config_file in config_files:
        if os.path.exists(config_file):
            print_status(f"é…ç½®æ–‡ä»¶: {config_file}", "success")
        else:
            print_status(f"é…ç½®æ–‡ä»¶: {config_file} - ç¼ºå¤±", "warning")
            missing_config.append(config_file)
    
    # å†³å®šæ˜¯å¦éœ€è¦é‡æ–°ä¸‹è½½
    if weights_complete and tokenizer_ok and not missing_config:
        print_status("æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œåªéœ€éªŒè¯åŠŸèƒ½", "success")
        return {"need_download": False, "missing_files": []}
    elif weights_complete and (not tokenizer_ok or missing_config):
        print_status("æƒé‡æ–‡ä»¶å®Œæ•´ï¼Œåªéœ€ä¸‹è½½é…ç½®æ–‡ä»¶", "info")
        missing_files = ["tokenizer.model"] if not tokenizer_ok else []
        missing_files.extend(missing_config)
        return {"need_download": "partial", "missing_files": missing_files}
    else:
        print_status("æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œéœ€è¦å®Œå…¨é‡æ–°ä¸‹è½½", "warning")
        return {"need_download": True, "missing_files": []}

def test_tokenizer(model_dir):
    """æµ‹è¯•tokenizeråŠŸèƒ½"""
    tokenizer_path = os.path.join(model_dir, "tokenizer.model")
    
    try:
        import sentencepiece as spm
        
        print_status("æµ‹è¯•tokenizeråŠŸèƒ½...", "progress")
        
        # åŠ è½½tokenizer
        sp = spm.SentencePieceProcessor()
        sp.load(tokenizer_path)
        
        # æµ‹è¯•ç¼–ç è§£ç 
        test_texts = [
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "ChatGLM3 is a conversational language model.",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿã€‚",
            "What is the meaning of life?"
        ]
        
        for text in test_texts:
            tokens = sp.encode(text)
            decoded = sp.decode(tokens)
            
            if decoded.strip() != text.strip():
                print_status(f"ç¼–ç è§£ç ä¸ä¸€è‡´: '{text}' -> '{decoded}'", "warning")
            else:
                print_status(f"æµ‹è¯•é€šè¿‡: '{text}' ({len(tokens)} tokens)", "success")
        
        print_status("TokenizeråŠŸèƒ½éªŒè¯é€šè¿‡!", "success")
        return True
        
    except ImportError:
        print_status("sentencepieceæœªå®‰è£…", "error")
        print_status("è¯·å®‰è£…: pip install sentencepiece", "info")
        return False
    except Exception as e:
        print_status(f"Tokenizeræµ‹è¯•å¤±è´¥: {e}", "error")
        return False

def main():
    print("ğŸš€ ChatGLM3 æ™ºèƒ½ä¸‹è½½å™¨")
    print("é€‚ç”¨äºç‡§åŸT20ç¯å¢ƒ - æ™ºèƒ½æ£€æŸ¥é¿å…é‡å¤ä¸‹è½½")
    print("=" * 50)
    
    # ç¡®å®šæ¨¡å‹ç›®å½•
    script_dir = Path(__file__).parent
    model_dir = script_dir.parent / "models" / "THUDM" / "chatglm3-6b"
    
    print_status(f"ç›®æ ‡ç›®å½•: {model_dir}", "info")
    
    # æ™ºèƒ½æ£€æŸ¥ç°æœ‰æ¨¡å‹
    check_result = check_existing_model(model_dir)
    
    # åˆ›å»ºç›®å½•
    model_dir.mkdir(parents=True, exist_ok=True)
    os.chdir(model_dir)
    
    print_status(f"å·¥ä½œç›®å½•: {os.getcwd()}", "info")
    
    # å®šä¹‰éœ€è¦ä¸‹è½½çš„æ–‡ä»¶
    files_to_download = {
        "config.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/config.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=config.json"
            ],
            "min_size": 100,
            "critical": True
        },
        "tokenizer_config.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer_config.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenizer_config.json"
            ],
            "min_size": 100,
            "critical": True
        },
        "special_tokens_map.json": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/special_tokens_map.json",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=special_tokens_map.json"
            ],
            "min_size": 10,
            "critical": True
        },
        "tokenizer.model": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenizer.model",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenizer.model"
            ],
            "min_size": 1000000,  # è‡³å°‘1MB
            "critical": True
        },
        "modeling_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/modeling_chatglm.py",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=modeling_chatglm.py"
            ],
            "min_size": 10000,
            "critical": True
        },
        "tokenization_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/tokenization_chatglm.py", 
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=tokenization_chatglm.py"
            ],
            "min_size": 5000,
            "critical": True
        },
        "configuration_chatglm.py": {
            "urls": [
                "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/configuration_chatglm.py",
                "https://www.modelscope.cn/api/v1/models/ZhipuAI/chatglm3-6b/repo?Revision=master&FilePath=configuration_chatglm.py"
            ],
            "min_size": 1000,
            "critical": True
        }
    }
    
    # æ ¹æ®æ£€æŸ¥ç»“æœå†³å®šä¸‹è½½ç­–ç•¥
    if check_result["need_download"] == False:
        print_status("ğŸ‰ æ‰€æœ‰æ–‡ä»¶éƒ½å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œè·³è¿‡ä¸‹è½½", "success")
        success_count = len(files_to_download)
        total_files = len(files_to_download)
    elif check_result["need_download"] == "partial":
        print_status(f"ğŸ“‹ åªéœ€è¦ä¸‹è½½éƒ¨åˆ†æ–‡ä»¶: {check_result['missing_files']}", "info")
        # åªä¸‹è½½ç¼ºå¤±çš„æ–‡ä»¶
        files_to_download = {k: v for k, v in files_to_download.items() 
                           if k in check_result['missing_files']}
        success_count = 0
        total_files = len(files_to_download)
    else:
        print_status("ğŸ“¥ éœ€è¦ä¸‹è½½æ‰€æœ‰æ–‡ä»¶", "info")
        success_count = 0
        total_files = len(files_to_download)
    
    # ä¸‹è½½æ–‡ä»¶
    if check_result["need_download"] != False:
        for filename, file_info in files_to_download.items():
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
            if verify_file_integrity(filename, file_info["min_size"]):
                print_status(f"{filename} å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡ä¸‹è½½", "success")
                success_count += 1
                continue
        
            # å°è¯•ä¸‹è½½
            downloaded = False
            for i, url in enumerate(file_info["urls"]):
                print_status(f"å°è¯•æº {i+1}/{len(file_info['urls'])}: {url.split('/')[-1]}", "progress")
                
                if download_file_with_progress(url, filename):
                    if verify_file_integrity(filename, file_info["min_size"]):
                        downloaded = True
                        success_count += 1
                        break
                    else:
                        print_status("ä¸‹è½½çš„æ–‡ä»¶æ— æ•ˆï¼Œåˆ é™¤å¹¶å°è¯•ä¸‹ä¸€ä¸ªæº", "warning")
                        try:
                            os.remove(filename)
                        except:
                            pass
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            if not downloaded:
                print_status(f"æ‰€æœ‰æºéƒ½å¤±è´¥: {filename}", "error")
                if file_info["critical"]:
                    print_status("è¿™æ˜¯å…³é”®æ–‡ä»¶ï¼Œä¸‹è½½å¤±è´¥å¯èƒ½å½±å“è®­ç»ƒ", "error")
    
    # æ€»ç»“
    print(f"\n{'='*50}")
    print_status(f"ä¸‹è½½å®Œæˆ: {success_count}/{total_files} æ–‡ä»¶æˆåŠŸ", "info")
    
    if success_count == total_files:
        print_status("æ‰€æœ‰æ–‡ä»¶ä¸‹è½½æˆåŠŸ!", "success")
        
        # æµ‹è¯•tokenizer
        if test_tokenizer(model_dir):
            print_status("ChatGLM3æ¨¡å‹å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒ!", "success")
            return 0
        else:
            print_status("Tokenizeræµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥", "error")
            return 1
    else:
        print_status("éƒ¨åˆ†æ–‡ä»¶ä¸‹è½½å¤±è´¥", "error")
        return 1

if __name__ == "__main__":
    sys.exit(main())
