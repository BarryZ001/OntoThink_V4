#!/bin/bash
# ChatGLM3-6B å¤šæºä¸‹è½½è„šæœ¬

set -e

MODEL_DIR="enflame_training/models/THUDM/chatglm3-6b"
BASE_DIR=$(dirname "$0")/../..

echo "ğŸš€ ChatGLM3-6B å¤šæºä¸‹è½½è„šæœ¬"
echo "ç›®æ ‡ç›®å½•: $MODEL_DIR"

# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p "$BASE_DIR/$MODEL_DIR"
cd "$BASE_DIR/$MODEL_DIR"

echo ""
echo "ğŸ“ å¯é€‰çš„ä¸‹è½½æ–¹å¼:"
echo "1. HuggingFaceå®˜æ–¹æº (å›½å¤–)"
echo "2. ModelScopeé•œåƒæº (å›½å†…ï¼Œæ¨è)"
echo "3. æ‰‹åŠ¨ä¸‹è½½æ ¸å¿ƒæ–‡ä»¶"
echo "4. ä½¿ç”¨Pythonä¸‹è½½"
echo ""

read -p "è¯·é€‰æ‹©ä¸‹è½½æ–¹å¼ (1-4): " choice

case $choice in
    1)
        echo "ğŸ“¥ ä½¿ç”¨HuggingFaceå®˜æ–¹æº..."
        git clone https://huggingface.co/THUDM/chatglm3-6b .
        ;;
    2)
        echo "ğŸ“¥ ä½¿ç”¨ModelScopeé•œåƒæº (æ¨è)..."
        echo "è®¾ç½®Gité…ç½®ç”¨äºModelScope..."
        git config --global url."https://www.modelscope.cn/".insteadOf "https://huggingface.co/"
        git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git .
        echo "âœ… æ¢å¤Gité…ç½®..."
        git config --global --unset url."https://www.modelscope.cn/".insteadOf
        ;;
    3)
        echo "ğŸ“¥ æ‰‹åŠ¨ä¸‹è½½æ ¸å¿ƒæ–‡ä»¶..."
        
        # åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶
        cat > config.json << 'EOF'
{
  "_name_or_path": "THUDM/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "ffn_hidden_size": 13696,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}
EOF

        cat > tokenizer_config.json << 'EOF'
{
  "add_bos_token": false,
  "add_eos_token": false,
  "auto_map": {
    "AutoTokenizer": [
      "tokenization_chatglm.ChatGLMTokenizer",
      null
    ]
  },
  "clean_up_tokenization_spaces": true,
  "do_lower_case": false,
  "model_max_length": 1000000000000000019884624838656,
  "padding_side": "left",
  "remove_space": false,
  "tokenizer_class": "ChatGLMTokenizer",
  "trust_remote_code": true,
  "unk_token": "<unk>",
  "use_fast": false
}
EOF

        echo "ğŸ“¥ ä¸‹è½½å…³é”®æ–‡ä»¶..."
        echo "âš ï¸  æ³¨æ„ï¼šæ¨¡å‹æƒé‡æ–‡ä»¶å¾ˆå¤§ï¼Œéœ€è¦æ‰‹åŠ¨ä¸‹è½½"
        echo "è¯·ä»ä»¥ä¸‹åœ°å€æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶:"
        echo "1. ModelScope: https://www.modelscope.cn/ZhipuAI/chatglm3-6b/files"
        echo "2. HuggingFace: https://huggingface.co/THUDM/chatglm3-6b/tree/main"
        echo ""
        echo "éœ€è¦ä¸‹è½½çš„æ–‡ä»¶:"
        echo "- pytorch_model-00001-of-00007.bin ~ pytorch_model-00007-of-00007.bin"
        echo "- pytorch_model.bin.index.json"
        echo "- tokenizer.model"
        echo "- modeling_chatglm.py"
        echo "- tokenization_chatglm.py"
        echo "- configuration_chatglm.py"
        ;;
    4)
        echo "ğŸ“¥ ä½¿ç”¨Pythonä¸‹è½½..."
        python3 << 'EOF'
import os
try:
    from modelscope import snapshot_download
    print("ä½¿ç”¨ModelScopeä¸‹è½½...")
    model_dir = snapshot_download('ZhipuAI/chatglm3-6b', cache_dir='.')
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_dir}")
except ImportError:
    print("ModelScopeæœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨transformers...")
    try:
        from transformers import AutoModel, AutoTokenizer
        print("ä½¿ç”¨transformersä¸‹è½½...")
        model = AutoModel.from_pretrained('THUDM/chatglm3-6b', trust_remote_code=True)
        tokenizer = AutoTokenizer.from_pretrained('THUDM/chatglm3-6b', trust_remote_code=True)
        model.save_pretrained('.')
        tokenizer.save_pretrained('.')
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("è¯·å°è¯•æ‰‹åŠ¨ä¸‹è½½æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼")
EOF
        ;;
    *)
        echo "âŒ æ— æ•ˆé€‰æ‹©"
        exit 1
        ;;
esac

echo ""
echo "ğŸ” æ£€æŸ¥ä¸‹è½½ç»“æœ..."
if [ -f "config.json" ] && [ -f "tokenizer_config.json" ]; then
    echo "âœ… åŸºç¡€é…ç½®æ–‡ä»¶å­˜åœ¨"
    
    if ls pytorch_model*.bin >/dev/null 2>&1 || [ -f "pytorch_model.bin" ]; then
        echo "âœ… æ¨¡å‹æƒé‡æ–‡ä»¶å­˜åœ¨"
        echo "ğŸ‰ ChatGLM3-6Bä¸‹è½½å®Œæˆï¼"
    else
        echo "âš ï¸  æ¨¡å‹æƒé‡æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½"
        echo "æˆ–é‡æ–°è¿è¡Œè„šæœ¬é€‰æ‹©å…¶ä»–ä¸‹è½½æ–¹å¼"
    fi
else
    echo "âŒ ä¸‹è½½æœªå®Œæˆï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å°è¯•å…¶ä»–æ–¹å¼"
fi

echo ""
echo "ğŸ“‹ ä¸‹è½½å®Œæˆåçš„ä¸‹ä¸€æ­¥:"
echo "cd /workspace/code/OntoThink_V4"
echo "python3 enflame_training/scripts/train_ontothink_enflame.py --step full"
