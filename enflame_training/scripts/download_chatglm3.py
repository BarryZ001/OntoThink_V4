#!/usr/bin/env python3
"""
ChatGLM3-6B å¿«é€Ÿä¸‹è½½è„šæœ¬
æ”¯æŒå¤šä¸ªé•œåƒæºï¼Œé€‚åˆå›½å†…ç½‘ç»œç¯å¢ƒ
"""

import os
import sys
import subprocess
from pathlib import Path

def download_with_modelscope():
    """ä½¿ç”¨ModelScopeä¸‹è½½ï¼ˆå›½å†…æ¨èï¼‰"""
    try:
        print("ğŸ“¥ å°è¯•ä½¿ç”¨ModelScopeä¸‹è½½...")
        
        # å…ˆå°è¯•å®‰è£…modelscope
        subprocess.run([sys.executable, "-m", "pip", "install", "modelscope"], 
                      capture_output=True, check=False)
        
        from modelscope import snapshot_download
        
        model_dir = snapshot_download(
            'ZhipuAI/chatglm3-6b',
            cache_dir='.',
            revision='master'
        )
        
        print(f"âœ… ModelScopeä¸‹è½½å®Œæˆ: {model_dir}")
        return True
        
    except Exception as e:
        print(f"âŒ ModelScopeä¸‹è½½å¤±è´¥: {e}")
        return False

def download_with_transformers():
    """ä½¿ç”¨transformersä¸‹è½½"""
    try:
        print("ğŸ“¥ å°è¯•ä½¿ç”¨transformersä¸‹è½½...")
        
        from transformers import AutoModel, AutoTokenizer
        
        print("ğŸ“¥ ä¸‹è½½æ¨¡å‹...")
        model = AutoModel.from_pretrained(
            'THUDM/chatglm3-6b', 
            trust_remote_code=True,
            torch_dtype='auto'
        )
        
        print("ğŸ“¥ ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            'THUDM/chatglm3-6b', 
            trust_remote_code=True
        )
        
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        model.save_pretrained('.')
        tokenizer.save_pretrained('.')
        
        print("âœ… transformersä¸‹è½½å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ transformersä¸‹è½½å¤±è´¥: {e}")
        return False

def download_with_git_mirror():
    """ä½¿ç”¨Gité•œåƒä¸‹è½½"""
    try:
        print("ğŸ“¥ å°è¯•ä½¿ç”¨Gité•œåƒä¸‹è½½...")
        
        # ä½¿ç”¨å›½å†…é•œåƒ
        mirror_urls = [
            "https://hf-mirror.com/THUDM/chatglm3-6b",
            "https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git"
        ]
        
        for url in mirror_urls:
            try:
                print(f"ğŸ“¥ å°è¯•ä» {url} ä¸‹è½½...")
                result = subprocess.run(
                    ["git", "clone", url, "."],
                    capture_output=True,
                    text=True,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                if result.returncode == 0:
                    print(f"âœ… Gité•œåƒä¸‹è½½å®Œæˆ: {url}")
                    return True
                else:
                    print(f"âŒ ä» {url} ä¸‹è½½å¤±è´¥: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                print(f"â° ä» {url} ä¸‹è½½è¶…æ—¶")
                
    except Exception as e:
        print(f"âŒ Gité•œåƒä¸‹è½½å¤±è´¥: {e}")
        
    return False

def create_basic_config():
    """åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰"""
    print("ğŸ“ åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶...")
    
    config = {
        "_name_or_path": "THUDM/chatglm3-6b",
        "architectures": ["ChatGLMModel"],
        "auto_map": {
            "AutoConfig": "configuration_chatglm.ChatGLMConfig",
            "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
            "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration"
        },
        "hidden_size": 4096,
        "num_layers": 28,
        "num_attention_heads": 32,
        "vocab_size": 65024,
        "torch_dtype": "float16",
        "transformers_version": "4.30.2"
    }
    
    import json
    with open('config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    tokenizer_config = {
        "auto_map": {
            "AutoTokenizer": ["tokenization_chatglm.ChatGLMTokenizer", None]
        },
        "tokenizer_class": "ChatGLMTokenizer",
        "trust_remote_code": True
    }
    
    with open('tokenizer_config.json', 'w', encoding='utf-8') as f:
        json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
    
    print("âœ… åŸºç¡€é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ")

def main():
    print("ğŸš€ ChatGLM3-6B å¤šæºä¸‹è½½å™¨")
    print("=" * 50)
    
    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
    model_dir = Path(__file__).parent.parent / "models" / "THUDM" / "chatglm3-6b"
    
    # å¦‚æœç›®å½•å­˜åœ¨ä½†ä¸ºç©ºæˆ–ä¸å®Œæ•´ï¼Œå…ˆæ¸…ç†
    if model_dir.exists():
        files = list(model_dir.glob("*"))
        if not files or not any(f.name == "config.json" for f in files):
            print(f"ğŸ§¹ æ¸…ç†ä¸å®Œæ•´çš„ç›®å½•: {model_dir}")
            import shutil
            shutil.rmtree(model_dir)
    
    model_dir.mkdir(parents=True, exist_ok=True)
    os.chdir(model_dir)
    
    print(f"ğŸ“ ç›®æ ‡ç›®å½•: {model_dir}")
    
    # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„ä¸‹è½½æ–¹å¼
    download_methods = [
        ("ModelScope (å›½å†…æ¨è)", download_with_modelscope),
        ("Gité•œåƒ", download_with_git_mirror),
        ("Transformers", download_with_transformers)
    ]
    
    for method_name, method_func in download_methods:
        print(f"\nğŸ”„ å°è¯•æ–¹å¼: {method_name}")
        
        if method_func():
            print(f"ğŸ‰ ä¸‹è½½æˆåŠŸï¼ä½¿ç”¨æ–¹å¼: {method_name}")
            break
    else:
        print("\nâŒ æ‰€æœ‰ä¸‹è½½æ–¹å¼éƒ½å¤±è´¥äº†")
        print("ğŸ“ åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶ä½œä¸ºä¸´æ—¶æ–¹æ¡ˆ...")
        create_basic_config()
        
        print("\nğŸ’¡ æ‰‹åŠ¨ä¸‹è½½å»ºè®®:")
        print("1. è®¿é—® https://www.modelscope.cn/ZhipuAI/chatglm3-6b/files")
        print("2. ä¸‹è½½æ‰€æœ‰ pytorch_model*.bin æ–‡ä»¶")
        print("3. ä¸‹è½½ tokenizer.model å’Œç›¸å…³Pythonæ–‡ä»¶")
        print("4. æ”¾ç½®åˆ°å½“å‰ç›®å½•")
        
        return False
    
    # éªŒè¯ä¸‹è½½ç»“æœ
    print("\nğŸ” éªŒè¯ä¸‹è½½ç»“æœ...")
    required_files = ['config.json', 'tokenizer_config.json']
    model_files = list(Path('.').glob('pytorch_model*.bin'))
    
    all_good = True
    for file in required_files:
        if Path(file).exists():
            print(f"âœ… {file}")
        else:
            print(f"âŒ {file}")
            all_good = False
    
    if model_files:
        print(f"âœ… æ¨¡å‹æƒé‡æ–‡ä»¶: {len(model_files)} ä¸ª")
    else:
        print("âŒ æ¨¡å‹æƒé‡æ–‡ä»¶ç¼ºå¤±")
        all_good = False
    
    if all_good:
        print("\nğŸ‰ ChatGLM3-6B ä¸‹è½½å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("cd /workspace/code/OntoThink_V4")
        print("python3 enflame_training/scripts/train_ontothink_enflame.py --step full")
    else:
        print("\nâš ï¸  ä¸‹è½½ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥æˆ–é‡è¯•")
    
    return all_good

if __name__ == "__main__":
    main()
