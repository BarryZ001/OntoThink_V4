#!/bin/bash

# ğŸ”§ è¶…çº§å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
# ä½¿ç”¨æœ€æ¿€è¿›çš„å†…å­˜èŠ‚çœç­–ç•¥
# ============================

echo "ğŸ”§ è¶…çº§å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"
echo "ä½¿ç”¨æœ€æ¿€è¿›çš„å†…å­˜èŠ‚çœç­–ç•¥"
echo "========================"

echo ""
echo "ğŸ¯ å½“å‰è¿›å±•ï¼š"
echo "âœ… å‚æ•°é”™è¯¯å·²è§£å†³"
echo "âœ… ECCLåˆå§‹åŒ–æˆåŠŸ"
echo "âœ… DeepSpeedå¯åŠ¨æ­£å¸¸"
echo "âŒ ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜åˆ†é…å¤±è´¥"

echo ""
echo "ğŸ”§ è¶…æ¿€è¿›å†…å­˜ä¼˜åŒ–ç­–ç•¥"
echo "===================="

# ç‡§åŸè„šæœ¬ç›®å½•
ENFLAME_SCRIPT_DIR="/workspace/code/OntoThink_V4/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"

if [ ! -f "$ENFLAME_SCRIPT_DIR/finetune_chatglm3_for_multiturn.py" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨"
    exit 1
fi

echo "âœ… æ‰¾åˆ°è®­ç»ƒè„šæœ¬: $ENFLAME_SCRIPT_DIR"
cd "$ENFLAME_SCRIPT_DIR"

# è®¾ç½®ç‡§åŸåˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=false  # ç¦ç”¨åŸåœ°æ“ä½œä»¥èŠ‚çœå†…å­˜
export OMP_NUM_THREADS=1  # æœ€å°çº¿ç¨‹æ•°
export ECCL_MAX_NCHANNELS=1
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=3"  # å¢åŠ é‡è¯•æ¬¡æ•°

# åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼ˆå•å¡é…ç½®ï¼‰
export MASTER_ADDR=localhost
export MASTER_PORT=29500
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0
export GCU_VISIBLE_DEVICES=0

# ç‡§åŸç‰¹å®šçš„åˆ†å¸ƒå¼å˜é‡
export CUDA_VISIBLE_DEVICES=""
export PTEX_DDP_BACKEND=eccl

# è®¾ç½®æ›´å°çš„å†…å­˜æ± 
export ECCL_BUFFSIZE=16777216  # 16MB instead of default 32MB

echo "ğŸ¯ è¶…æ¿€è¿›å†…å­˜ä¼˜åŒ–å‚æ•°ï¼š"
echo "  - max_tokens: 256 â†’ 128 (æå°åºåˆ—é•¿åº¦)"
echo "  - gradient_accumulation: 2 â†’ 1 (æœ€å°ç´¯ç§¯)"
echo "  - max_steps: 3 â†’ 1 (å•æ­¥æµ‹è¯•)"
echo "  - OMP_NUM_THREADS: 2 â†’ 1 (å•çº¿ç¨‹)"
echo "  - ENFLAME_PT_ENABLE_HBM_INPLACE: false (ç¦ç”¨åŸåœ°æ“ä½œ)"
echo "  - ECCL_BUFFSIZE: 16MB (å‡å°‘é€šä¿¡ç¼“å†²)"

echo ""
echo "ğŸš€ å¯åŠ¨è¶…çº§å†…å­˜ä¼˜åŒ–è®­ç»ƒ"
echo "======================="

# å¯åŠ¨æœ€å°å†…å­˜é…ç½®è®­ç»ƒ
python3.8 finetune_chatglm3_for_multiturn.py \
    --model_path "/workspace/code/OntoThink_V4/enflame_training/models/THUDM/chatglm3-6b" \
    --train_file "/workspace/code/OntoThink_V4/enflame_training/datasets/ontothink_multiturn/train.jsonl" \
    --tp_size 1 \
    --dp_size 1 \
    --pp_size 1 \
    --train_micro_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --max_steps 1 \
    --max_tokens 128 \
    --ladder_shape False \
    --skip_steps 1 \
    --eval_batch_size 1 \
    --eval_per_n_epochs 10 \
    --train_epochs 1 2>&1 | tee /tmp/memory_ultra_optimized.log

echo ""
echo "ğŸ” è¶…çº§ä¼˜åŒ–ç»“æœåˆ†æ"
echo "=================="

if [ -f /tmp/memory_ultra_optimized.log ]; then
    echo "ğŸ“‹ æ£€æŸ¥æœ€æ–°çš„è¾“å‡º:"
    tail -15 /tmp/memory_ultra_optimized.log
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥å†…å­˜åˆ†é…ï¼š"
    if grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess" /tmp/memory_ultra_optimized.log; then
        echo "âŒ ä»æœ‰å†…å­˜åˆ†é…é—®é¢˜ - éœ€è¦æ›´æ¿€è¿›çš„æ–¹æ¡ˆ"
        # æ£€æŸ¥å…·ä½“çš„å†…å­˜å¤§å°
        if grep -q -i "nbytes" /tmp/memory_ultra_optimized.log; then
            echo "ğŸ“Š å°è¯•åˆ†é…çš„å†…å­˜å¤§å°:"
            grep -i "nbytes\|topsMalloc" /tmp/memory_ultra_optimized.log | tail -3
        fi
    else
        echo "âœ… å†…å­˜åˆ†é…é—®é¢˜å·²è§£å†³ï¼"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥è®­ç»ƒæ˜¯å¦å¼€å§‹ï¼š"
    if grep -q -i "step.*loss\|epoch.*step\|loss.*:\|training.*start" /tmp/memory_ultra_optimized.log; then
        echo "ğŸ‰ è®­ç»ƒæˆåŠŸå¼€å§‹ï¼"
        grep -i "step.*loss\|epoch.*step\|loss.*:\|training.*start" /tmp/memory_ultra_optimized.log | tail -3
    elif grep -q -i "loading.*data\|processing.*data\|dataset" /tmp/memory_ultra_optimized.log; then
        echo "âš ï¸  æ­£åœ¨å¤„ç†æ•°æ®..."
        grep -i "loading.*data\|processing.*data\|dataset" /tmp/memory_ultra_optimized.log | tail -2
    else
        echo "âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¼€å§‹ä¿¡æ¯"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥ä¼˜åŒ–å™¨çŠ¶æ€ï¼š"
    if grep -q -i "optimizer.*success\|optimizer.*complete\|DeepSpeed.*initialized" /tmp/memory_ultra_optimized.log; then
        echo "âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ"
    elif grep -q -i "Creating.*optimizer\|DeepSpeed.*Basic.*Optimizer" /tmp/memory_ultra_optimized.log; then
        echo "âš ï¸  ä¼˜åŒ–å™¨æ­£åœ¨åˆå§‹åŒ–..."
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥ECCLçŠ¶æ€ï¼š"
    ECCL_COUNT=$(grep -c "ecclCommInitRank.*success" /tmp/memory_ultra_optimized.log)
    echo "âœ… ECCLåˆå§‹åŒ–æ¬¡æ•°: $ECCL_COUNT"
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼š"
    if grep -q -i "error\|fail\|exception\|abort" /tmp/memory_ultra_optimized.log; then
        echo "âŒ å‘ç°é”™è¯¯:"
        grep -i -A1 "error\|fail\|exception\|abort" /tmp/memory_ultra_optimized.log | tail -4
    else
        echo "âœ… æ²¡æœ‰å‘ç°é”™è¯¯"
    fi
fi

echo ""
echo "ğŸ’¡ è¶…çº§ä¼˜åŒ–ç»“æœæ€»ç»“"
echo "=================="

if [ -f /tmp/memory_ultra_optimized.log ]; then
    # æ£€æŸ¥æ˜¯å¦å½»åº•æˆåŠŸ
    if grep -q -i "step.*loss\|loss.*:" /tmp/memory_ultra_optimized.log && ! grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess" /tmp/memory_ultra_optimized.log; then
        echo "ğŸ‰ğŸ‰ğŸ‰ å®Œå…¨æˆåŠŸï¼è®­ç»ƒæ­£å¸¸è¿è¡Œï¼"
        echo "âœ… å†…å­˜ä¼˜åŒ–æˆåŠŸ"
        echo "âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ"
        echo "âœ… è®­ç»ƒæ­¥éª¤æ­£å¸¸"
        echo ""
        echo "ğŸš€ ä¸‹ä¸€æ­¥å¯ä»¥é€æ­¥å¢åŠ é…ç½®ï¼š"
        echo "1. max_tokens: 128 â†’ 256 â†’ 512"
        echo "2. gradient_accumulation: 1 â†’ 2 â†’ 4"
        echo "3. max_steps: 1 â†’ 10 â†’ 100"
        echo "4. å°è¯•å¤šå¡å¹¶è¡Œ"
    elif ! grep -q -i "topsMalloc.*failed\|Check failed.*topsSuccess" /tmp/memory_ultra_optimized.log; then
        echo "ğŸ¯ å†…å­˜é—®é¢˜å·²è§£å†³ï¼"
        echo "âœ… æ²¡æœ‰å†…å­˜åˆ†é…é”™è¯¯"
        echo "âš ï¸  å¯èƒ½åœ¨å…¶ä»–é˜¶æ®µï¼ˆæ•°æ®å¤„ç†/è®­ç»ƒå¾ªç¯ï¼‰"
        echo "ğŸ’¡ è¿™æ˜¯é‡å¤§è¿›æ­¥ï¼å†…å­˜ç“¶é¢ˆå·²çªç ´"
    else
        echo "âš ï¸  å†…å­˜é—®é¢˜ä»ç„¶å­˜åœ¨"
        echo "ğŸ”§ å¯èƒ½éœ€è¦ï¼š"
        echo "1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é‡åŒ–ç‰ˆæœ¬"
        echo "2. æ£€æŸ¥ç³»ç»Ÿå†…å­˜å’Œäº¤æ¢ç©ºé—´"
        echo "3. å°è¯•CPU offloadç­–ç•¥"
    fi
fi

echo ""
echo "ğŸ“‹ è¶…çº§ä¼˜åŒ–é…ç½®:"
echo "  max_tokens=128, micro_batch=1, grad_accum=1, max_steps=1"
echo "  OMP_NUM_THREADS=1, ECCL_BUFFSIZE=16MB"
echo "  å®Œæ•´æ—¥å¿—: /tmp/memory_ultra_optimized.log"
