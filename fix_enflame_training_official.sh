#!/bin/bash

# ğŸ”¥ ç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒä¿®å¤è„šæœ¬
# åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£å’Œç¤ºä¾‹è„šæœ¬
# ========================================

echo "ğŸ”¥ ç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒä¿®å¤"
echo "åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£å’Œllm_scripts"
echo "========================================"

# ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$SCRIPT_DIR"

echo "ğŸ“ é¡¹ç›®æ ¹ç›®å½•: $PROJECT_ROOT"

# 1. è®¾ç½®ç‡§åŸT20ç¯å¢ƒå˜é‡ï¼ˆæ¥è‡ªå®˜æ–¹è„šæœ¬ï¼‰
echo ""
echo "ğŸ”§ 1. è®¾ç½®ç‡§åŸT20ç¯å¢ƒå˜é‡"
echo "----------------------------------------"

cat << 'EOF' > /tmp/enflame_env.sh
#!/bin/bash
# ç‡§åŸT20å®˜æ–¹ç¯å¢ƒå˜é‡è®¾ç½®

# ç‡§åŸEFPåŠ é€Ÿ
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true

# OpenMPè®¾ç½®
export OMP_NUM_THREADS=5

# ECCLåˆ†å¸ƒå¼é€šä¿¡è®¾ç½®
export ECCL_MAX_NCHANNELS=2

# ç‡§åŸå†…å­˜ç®¡ç†
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"

# GCUè®¾å¤‡å¯è§æ€§
export GCU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

echo "âœ… ç‡§åŸT20ç¯å¢ƒå˜é‡å·²è®¾ç½®"
EOF

echo "âœ… ç‡§åŸç¯å¢ƒå˜é‡è„šæœ¬å·²åˆ›å»º: /tmp/enflame_env.sh"

# 2. åˆ›å»ºåŸºäºç‡§åŸå®˜æ–¹çš„OntoThinkè®­ç»ƒè„šæœ¬
echo ""
echo "ğŸš€ 2. åˆ›å»ºç‡§åŸå®˜æ–¹æ ‡å‡†è®­ç»ƒè„šæœ¬"
echo "----------------------------------------"

cat << 'EOF' > "$PROJECT_ROOT/train_ontothink_enflame_official.sh"
#!/bin/bash
#
# ğŸ”¥ OntoThinkç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒè„šæœ¬
# åŸºäºç‡§åŸå®˜æ–¹chatglm3_6b_1h8c_multiturn.sh
#
set -eu -o pipefail

# åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$SCRIPT_DIR"

echo "ğŸ”¥ OntoThink ç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒ"
echo "åŸºäºç‡§åŸå®˜æ–¹chatglm3_6b_1h8c_multiturn.sh"
echo "============================================"

# ============================== ç‡§åŸç¯å¢ƒè®¾ç½® ================================
echo "ğŸ”§ è®¾ç½®ç‡§åŸT20ç¯å¢ƒå˜é‡..."

export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=5
export ECCL_MAX_NCHANNELS=2
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"
export GCU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

echo "âœ… ç‡§åŸç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"

# ============================== è®­ç»ƒå‚æ•°é…ç½® ================================
echo "ğŸ“‹ é…ç½®è®­ç»ƒå‚æ•°..."

# æ¨¡å‹è·¯å¾„
export PRETRAINED_MODEL_PATH="$PROJECT_ROOT/enflame_training/models/THUDM/chatglm3-6b"

# è®­ç»ƒæ•°æ®è·¯å¾„
export TRAIN_FILE="$PROJECT_ROOT/enflame_training/datasets/ontothink_multiturn/train.jsonl"

# è®­ç»ƒå‚æ•°ï¼ˆä½¿ç”¨ç‡§åŸå®˜æ–¹æ¨èå€¼ï¼‰
export MAX_TOKENS="2048"
export TP_SIZE="1"
export DP_SIZE="1"
export PP_SIZE="8"
export LADDER_SHAPE="False"
export SKIP_STEPS="10"
export MAX_STEPS="-1"
export MICRO_BATCH_SIZE="1"
export GARDIENT_ACCUMULATION_STEPS="64"
export EVAL_BATCH_SIZE="1"
export EVAL_PER_N_EPOCHS="1"
export TRAIN_EPOCHS="3"

echo "âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ"

# ============================== æ£€æŸ¥ä¾èµ– ================================
echo "ğŸ” æ£€æŸ¥ç‡§åŸè®­ç»ƒç¯å¢ƒ..."

# æ£€æŸ¥æ¨¡å‹
if [ ! -d "$PRETRAINED_MODEL_PATH" ]; then
    echo "âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: $PRETRAINED_MODEL_PATH"
    exit 1
fi

# æ£€æŸ¥è®­ç»ƒæ•°æ®
if [ ! -f "$TRAIN_FILE" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: $TRAIN_FILE"
    exit 1
fi

# æ£€æŸ¥ç‡§åŸè®­ç»ƒè„šæœ¬
ENFLAME_SCRIPT_DIR=""
for potential_dir in \
    "$PROJECT_ROOT/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3" \
    "/installer/topsrider_extracted/TopsRider_installer/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3" \
    "/usr/local/topsrider/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"; do
    if [ -d "$potential_dir" ] && [ -f "$potential_dir/finetune_chatglm3_for_multiturn.py" ]; then
        ENFLAME_SCRIPT_DIR="$potential_dir"
        break
    fi
done

if [ -z "$ENFLAME_SCRIPT_DIR" ]; then
    echo "âŒ æœªæ‰¾åˆ°ç‡§åŸChatGLM3è®­ç»ƒè„šæœ¬"
    exit 1
fi

echo "âœ… ç‡§åŸè„šæœ¬ç›®å½•: $ENFLAME_SCRIPT_DIR"

# ============================== è¾“å‡ºç›®å½•è®¾ç½® ================================
OUTPUT_DIR="$PROJECT_ROOT/enflame_training/models/ontothink-chatglm3-6b"
mkdir -p "$OUTPUT_DIR"

echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# ============================== å¯åŠ¨è®­ç»ƒ ================================
echo ""
echo "ğŸš€ å¯åŠ¨OntoThinkç‡§åŸT20è®­ç»ƒ..."
echo "è®­ç»ƒé…ç½®ï¼š"
echo "  æ¨¡å‹: $PRETRAINED_MODEL_PATH"
echo "  æ•°æ®: $TRAIN_FILE"
echo "  æœ€å¤§é•¿åº¦: $MAX_TOKENS"
echo "  å¹¶è¡Œ: TP=$TP_SIZE, DP=$DP_SIZE, PP=$PP_SIZE"
echo "  æ‰¹æ¬¡: micro=$MICRO_BATCH_SIZE, accum=$GARDIENT_ACCUMULATION_STEPS"
echo "  è½®æ•°: $TRAIN_EPOCHS"
echo "  è¾“å‡º: $OUTPUT_DIR"
echo ""

# åˆ‡æ¢åˆ°ç‡§åŸè„šæœ¬ç›®å½•
cd "$ENFLAME_SCRIPT_DIR"

# ä½¿ç”¨ç‡§åŸå®˜æ–¹å¯åŠ¨æ–¹å¼
python3.8 -u -m torch.distributed.launch \
    --nproc_per_node=8 \
    --standalone \
    --use_env finetune_chatglm3_for_multiturn.py \
    --model_path "$PRETRAINED_MODEL_PATH" \
    --train_file "$TRAIN_FILE" \
    --tp_size $TP_SIZE \
    --dp_size $DP_SIZE \
    --pp_size $PP_SIZE \
    --train_micro_batch_size $MICRO_BATCH_SIZE \
    --gradient_accumulation_steps $GARDIENT_ACCUMULATION_STEPS \
    --max_steps $MAX_STEPS \
    --max_tokens $MAX_TOKENS \
    --ladder_shape $LADDER_SHAPE \
    --skip_steps $SKIP_STEPS \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --eval_per_n_epochs $EVAL_PER_N_EPOCHS \
    --train_epochs $TRAIN_EPOCHS

echo "ğŸ‰ è®­ç»ƒå®Œæˆï¼"
EOF

chmod +x "$PROJECT_ROOT/train_ontothink_enflame_official.sh"
echo "âœ… ç‡§åŸå®˜æ–¹æ ‡å‡†è®­ç»ƒè„šæœ¬å·²åˆ›å»º"

# 3. åˆ›å»ºç‡§åŸä¾èµ–å®‰è£…è„šæœ¬
echo ""
echo "ğŸ“¦ 3. åˆ›å»ºç‡§åŸä¾èµ–å®‰è£…è„šæœ¬"
echo "----------------------------------------"

cat << 'EOF' > "$PROJECT_ROOT/install_enflame_official.sh"
#!/bin/bash

# ğŸ”¥ ç‡§åŸT20å®˜æ–¹ä¾èµ–å®‰è£…è„šæœ¬
# åŸºäºç‡§åŸå®˜æ–¹install_for_llm_scripts.sh
# ========================================

echo "ğŸ”¥ ç‡§åŸT20å®˜æ–¹ä¾èµ–å®‰è£…"
echo "åŸºäºç‡§åŸå®˜æ–¹install_for_llm_scripts.sh"
echo "=================================="

# ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$SCRIPT_DIR"

# æŸ¥æ‰¾ç‡§åŸå·¥å…·åŒ…
ENFLAME_ROOT=""
for potential_root in \
    "$PROJECT_ROOT/FromEnflame/ai_development_toolkit" \
    "/installer/topsrider_extracted/TopsRider_installer/ai_development_toolkit" \
    "/usr/local/topsrider/ai_development_toolkit"; do
    if [ -d "$potential_root/distributed" ] && [ -d "$potential_root/huggingface-gcu" ]; then
        ENFLAME_ROOT="$potential_root"
        break
    fi
done

if [ -z "$ENFLAME_ROOT" ]; then
    echo "âŒ æœªæ‰¾åˆ°ç‡§åŸå·¥å…·åŒ…"
    exit 1
fi

echo "âœ… ç‡§åŸå·¥å…·åŒ…: $ENFLAME_ROOT"

DIST_PATH="$ENFLAME_ROOT/distributed"
HF_PATH="$ENFLAME_ROOT/huggingface-gcu"
LLM_SCRIPTS_PATH="$DIST_PATH/llm_scripts_1.0.40"

# æ£€æŸ¥ç‡§åŸinstallè„šæœ¬
if [ ! -f "$LLM_SCRIPTS_PATH/install_for_llm_scripts.sh" ]; then
    echo "âŒ æœªæ‰¾åˆ°ç‡§åŸå®˜æ–¹å®‰è£…è„šæœ¬"
    exit 1
fi

echo "ğŸš€ è¿è¡Œç‡§åŸå®˜æ–¹ä¾èµ–å®‰è£…..."

# åˆ‡æ¢åˆ°ç‡§åŸè„šæœ¬ç›®å½•å¹¶è¿è¡Œå®˜æ–¹å®‰è£…
cd "$LLM_SCRIPTS_PATH"
bash install_for_llm_scripts.sh

echo "âœ… ç‡§åŸå®˜æ–¹ä¾èµ–å®‰è£…å®Œæˆ"

# é¢å¤–å®‰è£…ä¸€äº›å¯èƒ½éœ€è¦çš„åŒ…
echo "ğŸ“¦ å®‰è£…é¢å¤–ä¾èµ–..."
pip3 install sentencepiece==0.1.99 --no-deps
pip3 install einops==0.6.1 --no-deps
pip3 install rich --no-deps

echo "ğŸ‰ æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆï¼"
EOF

chmod +x "$PROJECT_ROOT/install_enflame_official.sh"
echo "âœ… ç‡§åŸå®˜æ–¹ä¾èµ–å®‰è£…è„šæœ¬å·²åˆ›å»º"

# 4. åˆ›å»ºä½¿ç”¨è¯´æ˜
echo ""
echo "ğŸ“– 4. åˆ›å»ºä½¿ç”¨è¯´æ˜"
echo "----------------------------------------"

cat << 'EOF' > "$PROJECT_ROOT/ENFLAME_TRAINING_GUIDE.md"
# ğŸ”¥ ç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒæŒ‡å—

åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£å’Œllm_scriptsç¤ºä¾‹

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ç‡§åŸä¾èµ–ï¼ˆæœåŠ¡å™¨ä¸Šè¿è¡Œï¼‰

```bash
cd /workspace/code/OntoThink_V4
bash install_enflame_official.sh
```

### 2. è¿è¡Œç‡§åŸå®˜æ–¹æ ‡å‡†è®­ç»ƒï¼ˆæœåŠ¡å™¨ä¸Šè¿è¡Œï¼‰

```bash
cd /workspace/code/OntoThink_V4
bash train_ontothink_enflame_official.sh
```

## ğŸ“‹ å…³é”®ç‰¹æ€§

### ğŸ”§ ç‡§åŸT20ç¯å¢ƒå˜é‡
- `ENFLAME_ENABLE_EFP=true`: å¯ç”¨ç‡§åŸEFPåŠ é€Ÿ
- `ENFLAME_PT_ENABLE_HBM_INPLACE=true`: å¯ç”¨HBMåŸåœ°æ“ä½œ
- `ECCL_MAX_NCHANNELS=2`: ECCLé€šä¿¡é€šé“æ•°
- `ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"`: å†…å­˜åˆ†é…é‡è¯•

### ğŸš€ ç‡§åŸåˆ†å¸ƒå¼å¯åŠ¨
- ä½¿ç”¨ `python3.8 -u -m torch.distributed.launch`
- `--nproc_per_node=8`: 8å¡GCU
- `--standalone`: å•æœºæ¨¡å¼
- `--use_env`: ä½¿ç”¨ç¯å¢ƒå˜é‡

### ğŸ“¦ ç‡§åŸå®˜æ–¹ä¾èµ–
- ä½¿ç”¨ç‡§åŸå®˜æ–¹ `install_for_llm_scripts.sh`
- ç‡§åŸä¼˜åŒ–ç‰ˆæœ¬ï¼šptex, collie_lm, deepspeed, transformers, accelerate, peft

## ğŸ” é—®é¢˜æ’æŸ¥

å¦‚æœè®­ç»ƒä»ç„¶å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š

1. **GCUè®¾å¤‡çŠ¶æ€**:
   ```bash
   ls -la /dev/gcu*
   ```

2. **ç‡§åŸPythonåŒ…**:
   ```bash
   python3 -c "import ptex, collie_lm; print('ç‡§åŸåŒ…æ­£å¸¸')"
   ```

3. **ç‡§åŸtorch_gcu**:
   ```bash
   python3 -c "import torch; print('PyTorch:', torch.__version__)"
   ```

## ğŸ“š å‚è€ƒæ–‡æ¡£

- ç‡§åŸLLMå¾®è°ƒç”¨æˆ·æŒ‡å—: `FromEnflame/.../documents/Enflame_llm_finetuning_user_guide.md`
- ç‡§åŸå®˜æ–¹ç¤ºä¾‹: `FromEnflame/.../llm_scripts_1.0.40/finetuning/chatglm3/`
EOF

echo "âœ… ä½¿ç”¨è¯´æ˜å·²åˆ›å»º: ENFLAME_TRAINING_GUIDE.md"

echo ""
echo "ğŸ‰ ç‡§åŸT20å®˜æ–¹æ ‡å‡†ä¿®å¤å®Œæˆï¼"
echo ""
echo "ğŸ“‹ æ¥ä¸‹æ¥è¯·åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼š"
echo "1. git pull origin main"
echo "2. bash install_enflame_official.sh"
echo "3. bash train_ontothink_enflame_official.sh"
echo ""
echo "è¿™ä¸ªè„šæœ¬å®Œå…¨åŸºäºç‡§åŸå®˜æ–¹æ ‡å‡†ï¼Œåº”è¯¥èƒ½è§£å†³è®­ç»ƒé—®é¢˜ï¼"
