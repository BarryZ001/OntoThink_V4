#!/usr/bin/env python3
"""
OntoThinkæ¨¡å‹éªŒè¯è„šæœ¬
ç”¨äºæµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹ç”Ÿæˆæ€è¾¨å›¾è°±çš„èƒ½åŠ›
"""

import json
import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
from typing import Dict, List

def load_model_and_tokenizer(model_path: str, base_model: str = "THUDM/chatglm3-6b"):
    """åŠ è½½è®­ç»ƒåçš„æ¨¡å‹å’Œtokenizer"""
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model,
        trust_remote_code=True,
        use_fast=False
    )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½PEFTé€‚é…å™¨
    model = PeftModel.from_pretrained(base_model, model_path)
    model = model.merge_and_unload()
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer

def generate_ontothink_response(model, tokenizer, question: str, max_length: int = 2048):
    """ç”ŸæˆOntoThinkæ€è¾¨å›ç­”"""
    prompt = f"""<|system|>
ä½ æ˜¯OntoThinkæ€è¾¨åŠ©æ‰‹ï¼Œä¸“é—¨åˆ†æå“²å­¦é—®é¢˜ï¼Œç”Ÿæˆå¤šç»´åº¦æ€è¾¨å›¾è°±ã€‚
<|user|>
è¯·åŸºäºä»¥ä¸‹å“²å­¦é—®é¢˜ï¼Œç”ŸæˆåŒ…å«ä¸åŒç«‹åœºã€æ”¯æŒè®ºæ®å’Œåé—®çš„æ€è¾¨å›¾è°±æ•°æ®ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š

é—®é¢˜ï¼š{question}
<|assistant|>
"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=0.7,
            top_p=0.8,
            do_sample=True,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    return response.strip()

def validate_json_structure(response: str) -> Dict:
    """éªŒè¯ç”Ÿæˆçš„JSONç»“æ„æ˜¯å¦ç¬¦åˆOntoThinkæ ¼å¼"""
    try:
        data = json.loads(response)
        required_keys = ["question", "standpoints", "counter_questions"]
        
        validation_result = {
            "valid_json": True,
            "has_required_keys": all(key in data for key in required_keys),
            "standpoints_count": len(data.get("standpoints", [])),
            "counter_questions_count": len(data.get("counter_questions", [])),
            "detailed_structure": {}
        }
        
        # éªŒè¯standpointsç»“æ„
        if "standpoints" in data:
            standpoints_valid = all(
                "id" in sp and "text" in sp and "arguments" in sp
                for sp in data["standpoints"]
            )
            validation_result["detailed_structure"]["standpoints_valid"] = standpoints_valid
        
        # éªŒè¯counter_questionsç»“æ„
        if "counter_questions" in data:
            cq_valid = all(
                "id" in cq and "text" in cq
                for cq in data["counter_questions"]
            )
            validation_result["detailed_structure"]["counter_questions_valid"] = cq_valid
        
        return validation_result
    
    except json.JSONDecodeError:
        return {
            "valid_json": False,
            "has_required_keys": False,
            "standpoints_count": 0,
            "counter_questions_count": 0,
            "detailed_structure": {}
        }

def run_validation(model, tokenizer, test_questions: List[str]) -> Dict:
    """è¿è¡Œå®Œæ•´çš„æ¨¡å‹éªŒè¯"""
    results = {
        "total_questions": len(test_questions),
        "successful_generations": 0,
        "valid_json_count": 0,
        "average_standpoints": 0,
        "average_counter_questions": 0,
        "detailed_results": []
    }
    
    total_standpoints = 0
    total_counter_questions = 0
    
    for i, question in enumerate(test_questions):
        print(f"ğŸ” éªŒè¯é—®é¢˜ {i+1}/{len(test_questions)}: {question[:50]}...")
        
        start_time = time.time()
        
        try:
            response = generate_ontothink_response(model, tokenizer, question)
            generation_time = time.time() - start_time
            
            validation = validate_json_structure(response)
            
            result = {
                "question": question,
                "response": response,
                "generation_time": generation_time,
                "validation": validation
            }
            
            if response:
                results["successful_generations"] += 1
            
            if validation["valid_json"]:
                results["valid_json_count"] += 1
                total_standpoints += validation["standpoints_count"]
                total_counter_questions += validation["counter_questions_count"]
            
            results["detailed_results"].append(result)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
            results["detailed_results"].append({
                "question": question,
                "response": "",
                "generation_time": 0,
                "validation": {"valid_json": False, "error": str(e)}
            })
    
    # è®¡ç®—å¹³å‡å€¼
    if results["valid_json_count"] > 0:
        results["average_standpoints"] = total_standpoints / results["valid_json_count"]
        results["average_counter_questions"] = total_counter_questions / results["valid_json_count"]
    
    return results

def main():
    parser = argparse.ArgumentParser(description="éªŒè¯OntoThinkæ¨¡å‹")
    parser.add_argument("--model_path", required=True, help="è®­ç»ƒåçš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_data_path", help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--output_path", required=True, help="éªŒè¯ç»“æœè¾“å‡ºè·¯å¾„")
    parser.add_argument("--base_model", default="THUDM/chatglm3-6b", help="åŸºç¡€æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(args.model_path, args.base_model)
    
    # å‡†å¤‡æµ‹è¯•é—®é¢˜
    if args.test_data_path:
        with open(args.test_data_path, 'r', encoding='utf-8') as f:
            test_data = [json.loads(line) for line in f]
            test_questions = [item.get("question", item.get("instruction", "")) for item in test_data[:10]]  # å–å‰10ä¸ª
    else:
        # ä½¿ç”¨é»˜è®¤æµ‹è¯•é—®é¢˜
        test_questions = [
            "äººå·¥æ™ºèƒ½æ˜¯å¦èƒ½å¤Ÿæ‹¥æœ‰çœŸæ­£çš„æ„è¯†ï¼Ÿ",
            "è‰ºæœ¯çš„ä»·å€¼æ˜¯ä¸»è§‚çš„è¿˜æ˜¯å®¢è§‚çš„ï¼Ÿ",
            "ä¸ªäººè‡ªç”±ä¸ç¤¾ä¼šç§©åºä¹‹é—´åº”è¯¥å¦‚ä½•å¹³è¡¡ï¼Ÿ",
            "ç§‘å­¦èƒ½å¦è§£é‡Šæ‰€æœ‰çš„è‡ªç„¶ç°è±¡ï¼Ÿ",
            "é“å¾·åˆ¤æ–­æ˜¯ç»å¯¹çš„è¿˜æ˜¯ç›¸å¯¹çš„ï¼Ÿ"
        ]
    
    print(f"ğŸ¯ å¼€å§‹éªŒè¯ï¼Œå…± {len(test_questions)} ä¸ªæµ‹è¯•é—®é¢˜")
    
    # è¿è¡ŒéªŒè¯
    results = run_validation(model, tokenizer, test_questions)
    
    # ä¿å­˜ç»“æœ
    with open(args.output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦:")
    print(f"   - æ€»é—®é¢˜æ•°: {results['total_questions']}")
    print(f"   - æˆåŠŸç”Ÿæˆ: {results['successful_generations']}")
    print(f"   - æœ‰æ•ˆJSON: {results['valid_json_count']}")
    print(f"   - å¹³å‡ç«‹åœºæ•°: {results['average_standpoints']:.1f}")
    print(f"   - å¹³å‡åé—®æ•°: {results['average_counter_questions']:.1f}")
    print(f"   - æˆåŠŸç‡: {results['valid_json_count']/results['total_questions']*100:.1f}%")
    
    print(f"\nâœ… éªŒè¯å®Œæˆï¼è¯¦ç»†ç»“æœä¿å­˜è‡³: {args.output_path}")

if __name__ == "__main__":
    main()
