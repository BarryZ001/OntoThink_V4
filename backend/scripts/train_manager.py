#!/usr/bin/env python3
"""
OntoThinkшонч╗ГчобчРЖхЩи
чФиф║Оч╗Яф╕АчобчРЖцХ┤ф╕кшонч╗Гц╡БчиЛ
"""

import subprocess
import argparse
import os
import json
import time
from pathlib import Path
from typing import Dict, Any
import shutil

class OntoThinkTrainingManager:
    def __init__(self, config_path: str = None):
        self.base_dir = Path("/Users/barryzhang/myDev3/OntoThink_V4")
        self.config = self.load_config(config_path)
        
    def load_config(self, config_path: str = None) -> Dict[str, Any]:
        """хКаш╜╜шонч╗ГщЕНч╜о"""
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # щ╗ШшодщЕНч╜о
        return {
            "model": {
                "base_model": "THUDM/chatglm3-6b",
                "output_dir": str(self.base_dir / "models" / "chatglm3-ontothink"),
                "max_seq_length": 2048
            },
            "data": {
                "raw_data_dir": str(self.base_dir / "backend" / "data" / "processed"),
                "optimized_data_dir": str(self.base_dir / "backend" / "data" / "optimized"),
                "expand_samples": 200
            },
            "training": {
                "num_gpus": 8,
                "batch_size_per_gpu": 2,
                "gradient_accumulation_steps": 4,
                "num_epochs": 3,
                "learning_rate": 5e-5,
                "use_lora": True,
                "lora_r": 64,
                "lora_alpha": 128,
                "q_lora": True
            },
            "deepseek": {
                "api_key": os.getenv("DEEPSEEK_API_KEY", "")
            }
        }
    
    def check_prerequisites(self) -> bool:
        """цгАцЯешонч╗ГхЙНцПРцЭбф╗╢"""
        print("ЁЯФН цгАцЯешонч╗ГхЙНцПРцЭбф╗╢...")
        
        # цгАцЯеGPU
        try:
            result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
            if result.returncode != 0:
                print("тЭМ цЬкцгАц╡ЛхИ░NVIDIA GPU")
                return False
            print("тЬЕ GPUцгАцЯещАЪш┐З")
        except FileNotFoundError:
            print("тЭМ nvidia-smiцЬкцЙ╛хИ░")
            return False
        
        # цгАцЯеPythonчОпхвГ
        required_packages = ["torch", "transformers", "peft", "datasets", "bitsandbytes"]
        for package in required_packages:
            try:
                __import__(package)
                print(f"тЬЕ {package} х╖▓хоЙшгЕ")
            except ImportError:
                print(f"тЭМ {package} цЬкхоЙшгЕ")
                return False
        
        # цгАцЯецХ░цНочЫох╜Х
        data_dir = Path(self.config["data"]["raw_data_dir"])
        if not data_dir.exists():
            print(f"тЭМ цХ░цНочЫох╜Хф╕НхнШхЬи: {data_dir}")
            return False
        
        print("тЬЕ хЙНцПРцЭбф╗╢цгАцЯехоМцИР")
        return True
    
    def expand_training_data(self) -> bool:
        """цЙйх▒Хшонч╗ГцХ░цНо"""
        print("ЁЯУИ х╝АхзЛцЙйх▒Хшонч╗ГцХ░цНо...")
        
        if not self.config["deepseek"]["api_key"]:
            print("тЪая╕П  цЬкщЕНч╜оDeepSeek APIхпЖщТея╝Мш╖│ш┐ЗцХ░цНоцЙйх▒Х")
            return True
        
        script_path = self.base_dir / "backend" / "scripts" / "expand_training_data.py"
        output_path = self.base_dir / "backend" / "data" / "expanded_data.jsonl"
        
        cmd = [
            "python", str(script_path),
            "--api_key", self.config["deepseek"]["api_key"],
            "--num_samples", str(self.config["data"]["expand_samples"]),
            "--output_path", str(output_path)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("тЬЕ цХ░цНоцЙйх▒ХхоМцИР")
                return True
            else:
                print(f"тЭМ цХ░цНоцЙйх▒Ххд▒ш┤е: {result.stderr}")
                return False
        except Exception as e:
            print(f"тЭМ цХ░цНоцЙйх▒Хх╝Вх╕╕: {e}")
            return False
    
    def prepare_optimized_data(self) -> bool:
        """хЗЖхдЗф╝ШхМЦчЪДшонч╗ГцХ░цНо"""
        print("ЁЯФз хЗЖхдЗф╝ШхМЦчЪДшонч╗ГцХ░цНо...")
        
        script_path = self.base_dir / "backend" / "scripts" / "prepare_optimized_data.py"
        
        cmd = [
            "python", str(script_path),
            "--input_dir", self.config["data"]["raw_data_dir"],
            "--output_dir", self.config["data"]["optimized_data_dir"]
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("тЬЕ цХ░цНоф╝ШхМЦхоМцИР")
                return True
            else:
                print(f"тЭМ цХ░цНоф╝ШхМЦхд▒ш┤е: {result.stderr}")
                return False
        except Exception as e:
            print(f"тЭМ цХ░цНоф╝ШхМЦх╝Вх╕╕: {e}")
            return False
    
    def start_training(self) -> bool:
        """хРпхКицибхЮЛшонч╗Г"""
        print("ЁЯЪА хРпхКицибхЮЛшонч╗Г...")
        
        # хИЫх╗║ш╛УхЗ║чЫох╜Х
        output_dir = Path(self.config["model"]["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ф┐ЭхнШшонч╗ГщЕНч╜о
        config_file = output_dir / "training_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        
        # цЮДх╗║шонч╗ГхС╜ф╗д
        training_script = self.base_dir / "backend" / "app" / "training" / "chatglm3_ontothink_training.py"
        
        cmd = [
            "torchrun",
            f"--nproc_per_node={self.config['training']['num_gpus']}",
            "--master_port=29500",
            str(training_script),
            "--model_name_or_path", self.config["model"]["base_model"],
            "--data_path", self.config["data"]["optimized_data_dir"],
            "--output_dir", self.config["model"]["output_dir"],
            "--num_train_epochs", str(self.config["training"]["num_epochs"]),
            "--per_device_train_batch_size", str(self.config["training"]["batch_size_per_gpu"]),
            "--gradient_accumulation_steps", str(self.config["training"]["gradient_accumulation_steps"]),
            "--learning_rate", str(self.config["training"]["learning_rate"]),
            "--max_seq_length", str(self.config["model"]["max_seq_length"]),
            "--use_lora", str(self.config["training"]["use_lora"]),
            "--lora_r", str(self.config["training"]["lora_r"]),
            "--lora_alpha", str(self.config["training"]["lora_alpha"]),
            "--q_lora", str(self.config["training"]["q_lora"]),
            "--bf16", "True",
            "--gradient_checkpointing", "True",
            "--evaluation_strategy", "steps",
            "--eval_steps", "100",
            "--save_strategy", "steps",
            "--save_steps", "200",
            "--logging_steps", "10",
            "--report_to", "tensorboard"
        ]
        
        # шо╛ч╜очОпхвГхПШщЗП
        env = os.environ.copy()
        env.update({
            "CUDA_VISIBLE_DEVICES": ",".join(map(str, range(self.config["training"]["num_gpus"]))),
            "NCCL_DEBUG": "INFO",
            "NCCL_IB_DISABLE": "1",
            "NCCL_P2P_DISABLE": "1"
        })
        
        # хРпхКишонч╗Г
        log_file = output_dir / f"training_{int(time.time())}.log"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                    env=env,
                    cwd=str(self.base_dir)
                )
                
                print(f"ЁЯУК шонч╗ГцЧех┐Ч: {log_file}")
                print("ЁЯФД шонч╗Гш┐ЫшбМф╕н...")
                
                # хоЮцЧ╢ш╛УхЗ║цЧех┐Ч
                for line in process.stdout:
                    print(line.rstrip())
                    f.write(line)
                    f.flush()
                
                process.wait()
                
                if process.returncode == 0:
                    print("тЬЕ шонч╗ГхоМцИР")
                    return True
                else:
                    print(f"тЭМ шонч╗Гхд▒ш┤ея╝Мш┐ФхЫЮчаБ: {process.returncode}")
                    return False
                    
        except Exception as e:
            print(f"тЭМ шонч╗Гх╝Вх╕╕: {e}")
            return False
    
    def validate_model(self) -> bool:
        """щкМшпБшонч╗ГхРОчЪДцибхЮЛ"""
        print("ЁЯФН щкМшпБшонч╗ГхРОчЪДцибхЮЛ...")
        
        script_path = self.base_dir / "backend" / "scripts" / "validate_model.py"
        model_path = self.config["model"]["output_dir"]
        test_data_path = Path(self.config["data"]["optimized_data_dir"]) / "test.jsonl"
        output_path = Path(model_path) / "validation_results.json"
        
        cmd = [
            "python", str(script_path),
            "--model_path", model_path,
            "--test_data_path", str(test_data_path) if test_data_path.exists() else "",
            "--output_path", str(output_path),
            "--base_model", self.config["model"]["base_model"]
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("тЬЕ цибхЮЛщкМшпБхоМцИР")
                print(result.stdout)
                return True
            else:
                print(f"тЭМ цибхЮЛщкМшпБхд▒ш┤е: {result.stderr}")
                return False
        except Exception as e:
            print(f"тЭМ цибхЮЛщкМшпБх╝Вх╕╕: {e}")
            return False
    
    def run_full_pipeline(self) -> bool:
        """ш┐РшбМхоМцХ┤чЪДшонч╗Гц╡БчиЛ"""
        print("ЁЯОп х╝АхзЛOntoThinkцибхЮЛхоМцХ┤шонч╗Гц╡БчиЛ")
        print("=" * 60)
        
        steps = [
            ("цгАцЯехЙНцПРцЭбф╗╢", self.check_prerequisites),
            ("цЙйх▒Хшонч╗ГцХ░цНо", self.expand_training_data),
            ("хЗЖхдЗф╝ШхМЦцХ░цНо", self.prepare_optimized_data),
            ("хРпхКицибхЮЛшонч╗Г", self.start_training),
            ("щкМшпБшонч╗ГцибхЮЛ", self.validate_model)
        ]
        
        for step_name, step_func in steps:
            print(f"\nЁЯУЛ {step_name}...")
            if not step_func():
                print(f"тЭМ {step_name}хд▒ш┤ея╝МхБЬцнвц╡БчиЛ")
                return False
            print(f"тЬЕ {step_name}хоМцИР")
        
        print("\nЁЯОЙ OntoThinkцибхЮЛшонч╗Гц╡БчиЛхЕищГихоМцИРя╝Б")
        print(f"ЁЯУБ цибхЮЛф┐ЭхнШф╜Нч╜о: {self.config['model']['output_dir']}")
        return True

def main():
    parser = argparse.ArgumentParser(description="OntoThinkшонч╗ГчобчРЖхЩи")
    parser.add_argument("--config", help="шонч╗ГщЕНч╜оцЦЗф╗╢ш╖пх╛Д")
    parser.add_argument("--step", choices=[
        "check", "expand", "prepare", "train", "validate", "full"
    ], default="full", help="цЙзшбМчЪДцнещкд")
    
    args = parser.parse_args()
    
    manager = OntoThinkTrainingManager(args.config)
    
    if args.step == "check":
        manager.check_prerequisites()
    elif args.step == "expand":
        manager.expand_training_data()
    elif args.step == "prepare":
        manager.prepare_optimized_data()
    elif args.step == "train":
        manager.start_training()
    elif args.step == "validate":
        manager.validate_model()
    elif args.step == "full":
        manager.run_full_pipeline()

if __name__ == "__main__":
    main()
