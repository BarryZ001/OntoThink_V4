#!/usr/bin/env python3
"""
OntoThinkè®­ç»ƒç®¡ç†å™¨
ç”¨äºç»Ÿä¸€ç®¡ç†æ•´ä¸ªè®­ç»ƒæµç¨‹
"""

import subprocess
import argparse
import os
import json
import time
from pathlib import Path
from typing import Dict, Any
import shutil

class OntoThinkTrainingManager:
    def __init__(self, config_path: str = None):
        self.base_dir = Path("/Users/barryzhang/myDev3/OntoThink_V4")
        self.config = self.load_config(config_path)
        
    def load_config(self, config_path: str = None) -> Dict[str, Any]:
        """åŠ è½½è®­ç»ƒé…ç½®"""
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # é»˜è®¤é…ç½®
        return {
            "model": {
                "base_model": "THUDM/chatglm3-6b",
                "output_dir": str(self.base_dir / "models" / "chatglm3-ontothink"),
                "max_seq_length": 2048
            },
            "data": {
                "raw_data_dir": str(self.base_dir / "backend" / "data" / "processed"),
                "optimized_data_dir": str(self.base_dir / "backend" / "data" / "optimized"),
                "expand_samples": 200
            },
            "training": {
                "num_gpus": 8,
                "batch_size_per_gpu": 2,
                "gradient_accumulation_steps": 4,
                "num_epochs": 3,
                "learning_rate": 5e-5,
                "use_lora": True,
                "lora_r": 64,
                "lora_alpha": 128,
                "q_lora": True
            },
            "deepseek": {
                "api_key": os.getenv("DEEPSEEK_API_KEY", "")
            }
        }
    
    def check_prerequisites(self) -> bool:
        """æ£€æŸ¥è®­ç»ƒå‰ææ¡ä»¶"""
        print("ğŸ” æ£€æŸ¥è®­ç»ƒå‰ææ¡ä»¶...")
        
        # æ£€æŸ¥GPU
        try:
            result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
            if result.returncode != 0:
                print("âŒ æœªæ£€æµ‹åˆ°NVIDIA GPU")
                return False
            print("âœ… GPUæ£€æŸ¥é€šè¿‡")
        except FileNotFoundError:
            print("âŒ nvidia-smiæœªæ‰¾åˆ°")
            return False
        
        # æ£€æŸ¥Pythonç¯å¢ƒ
        required_packages = ["torch", "transformers", "peft", "datasets", "bitsandbytes"]
        for package in required_packages:
            try:
                __import__(package)
                print(f"âœ… {package} å·²å®‰è£…")
            except ImportError:
                print(f"âŒ {package} æœªå®‰è£…")
                return False
        
        # æ£€æŸ¥æ•°æ®ç›®å½•
        data_dir = Path(self.config["data"]["raw_data_dir"])
        if not data_dir.exists():
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return False
        
        print("âœ… å‰ææ¡ä»¶æ£€æŸ¥å®Œæˆ")
        return True
    
    def expand_training_data(self) -> bool:
        """æ‰©å±•è®­ç»ƒæ•°æ®"""
        print("ğŸ“ˆ å¼€å§‹æ‰©å±•è®­ç»ƒæ•°æ®...")
        
        if not self.config["deepseek"]["api_key"]:
            print("âš ï¸  æœªé…ç½®DeepSeek APIå¯†é’¥ï¼Œè·³è¿‡æ•°æ®æ‰©å±•")
            return True
        
        script_path = self.base_dir / "backend" / "scripts" / "expand_training_data.py"
        output_path = self.base_dir / "backend" / "data" / "expanded_data.jsonl"
        
        cmd = [
            "python", str(script_path),
            "--api_key", self.config["deepseek"]["api_key"],
            "--num_samples", str(self.config["data"]["expand_samples"]),
            "--output_path", str(output_path)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("âœ… æ•°æ®æ‰©å±•å®Œæˆ")
                return True
            else:
                print(f"âŒ æ•°æ®æ‰©å±•å¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ æ•°æ®æ‰©å±•å¼‚å¸¸: {e}")
            return False
    
    def prepare_optimized_data(self) -> bool:
        """å‡†å¤‡ä¼˜åŒ–çš„è®­ç»ƒæ•°æ®"""
        print("ğŸ”§ å‡†å¤‡ä¼˜åŒ–çš„è®­ç»ƒæ•°æ®...")
        
        script_path = self.base_dir / "backend" / "scripts" / "prepare_optimized_data.py"
        
        cmd = [
            "python", str(script_path),
            "--input_dir", self.config["data"]["raw_data_dir"],
            "--output_dir", self.config["data"]["optimized_data_dir"]
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("âœ… æ•°æ®ä¼˜åŒ–å®Œæˆ")
                return True
            else:
                print(f"âŒ æ•°æ®ä¼˜åŒ–å¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ æ•°æ®ä¼˜åŒ–å¼‚å¸¸: {e}")
            return False
    
    def start_training(self) -> bool:
        """å¯åŠ¨æ¨¡å‹è®­ç»ƒ"""
        print("ğŸš€ å¯åŠ¨æ¨¡å‹è®­ç»ƒ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config["model"]["output_dir"])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config_file = output_dir / "training_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        training_script = self.base_dir / "backend" / "app" / "training" / "chatglm3_ontothink_training.py"
        
        cmd = [
            "torchrun",
            f"--nproc_per_node={self.config['training']['num_gpus']}",
            "--master_port=29500",
            str(training_script),
            "--model_name_or_path", self.config["model"]["base_model"],
            "--data_path", self.config["data"]["optimized_data_dir"],
            "--output_dir", self.config["model"]["output_dir"],
            "--num_train_epochs", str(self.config["training"]["num_epochs"]),
            "--per_device_train_batch_size", str(self.config["training"]["batch_size_per_gpu"]),
            "--gradient_accumulation_steps", str(self.config["training"]["gradient_accumulation_steps"]),
            "--learning_rate", str(self.config["training"]["learning_rate"]),
            "--max_seq_length", str(self.config["model"]["max_seq_length"]),
            "--use_lora", str(self.config["training"]["use_lora"]),
            "--lora_r", str(self.config["training"]["lora_r"]),
            "--lora_alpha", str(self.config["training"]["lora_alpha"]),
            "--q_lora", str(self.config["training"]["q_lora"]),
            "--bf16", "True",
            "--gradient_checkpointing", "True",
            "--evaluation_strategy", "steps",
            "--eval_steps", "100",
            "--save_strategy", "steps",
            "--save_steps", "200",
            "--logging_steps", "10",
            "--report_to", "tensorboard"
        ]
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env.update({
            "CUDA_VISIBLE_DEVICES": ",".join(map(str, range(self.config["training"]["num_gpus"]))),
            "NCCL_DEBUG": "INFO",
            "NCCL_IB_DISABLE": "1",
            "NCCL_P2P_DISABLE": "1"
        })
        
        # å¯åŠ¨è®­ç»ƒ
        log_file = output_dir / f"training_{int(time.time())}.log"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    universal_newlines=True,
                    env=env,
                    cwd=str(self.base_dir)
                )
                
                print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: {log_file}")
                print("ğŸ”„ è®­ç»ƒè¿›è¡Œä¸­...")
                
                # å®æ—¶è¾“å‡ºæ—¥å¿—
                for line in process.stdout:
                    print(line.rstrip())
                    f.write(line)
                    f.flush()
                
                process.wait()
                
                if process.returncode == 0:
                    print("âœ… è®­ç»ƒå®Œæˆ")
                    return True
                else:
                    print(f"âŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                    return False
                    
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¼‚å¸¸: {e}")
            return False
    
    def validate_model(self) -> bool:
        """éªŒè¯è®­ç»ƒåçš„æ¨¡å‹"""
        print("ğŸ” éªŒè¯è®­ç»ƒåçš„æ¨¡å‹...")
        
        script_path = self.base_dir / "backend" / "scripts" / "validate_model.py"
        model_path = self.config["model"]["output_dir"]
        test_data_path = Path(self.config["data"]["optimized_data_dir"]) / "test.jsonl"
        output_path = Path(model_path) / "validation_results.json"
        
        cmd = [
            "python", str(script_path),
            "--model_path", model_path,
            "--test_data_path", str(test_data_path) if test_data_path.exists() else "",
            "--output_path", str(output_path),
            "--base_model", self.config["model"]["base_model"]
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(self.base_dir))
            if result.returncode == 0:
                print("âœ… æ¨¡å‹éªŒè¯å®Œæˆ")
                print(result.stdout)
                return True
            else:
                print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ æ¨¡å‹éªŒè¯å¼‚å¸¸: {e}")
            return False
    
    def run_full_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ å¼€å§‹OntoThinkæ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        steps = [
            ("æ£€æŸ¥å‰ææ¡ä»¶", self.check_prerequisites),
            ("æ‰©å±•è®­ç»ƒæ•°æ®", self.expand_training_data),
            ("å‡†å¤‡ä¼˜åŒ–æ•°æ®", self.prepare_optimized_data),
            ("å¯åŠ¨æ¨¡å‹è®­ç»ƒ", self.start_training),
            ("éªŒè¯è®­ç»ƒæ¨¡å‹", self.validate_model)
        ]
        
        for step_name, step_func in steps:
            print(f"\nğŸ“‹ {step_name}...")
            if not step_func():
                print(f"âŒ {step_name}å¤±è´¥ï¼Œåœæ­¢æµç¨‹")
                return False
            print(f"âœ… {step_name}å®Œæˆ")
        
        print("\nğŸ‰ OntoThinkæ¨¡å‹è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {self.config['model']['output_dir']}")
        return True

def main():
    parser = argparse.ArgumentParser(description="OntoThinkè®­ç»ƒç®¡ç†å™¨")
    parser.add_argument("--config", help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--step", choices=[
        "check", "expand", "prepare", "train", "validate", "full"
    ], default="full", help="æ‰§è¡Œçš„æ­¥éª¤")
    
    args = parser.parse_args()
    
    manager = OntoThinkTrainingManager(args.config)
    
    if args.step == "check":
        manager.check_prerequisites()
    elif args.step == "expand":
        manager.expand_training_data()
    elif args.step == "prepare":
        manager.prepare_optimized_data()
    elif args.step == "train":
        manager.start_training()
    elif args.step == "validate":
        manager.validate_model()
    elif args.step == "full":
        manager.run_full_pipeline()

if __name__ == "__main__":
    main()
