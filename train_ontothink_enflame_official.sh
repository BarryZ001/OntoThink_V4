#!/bin/bash
#
# ğŸ”¥ OntoThinkç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒè„šæœ¬
# åŸºäºç‡§åŸå®˜æ–¹chatglm3_6b_1h8c_multiturn.sh
#
set -eu -o pipefail

# åŠ¨æ€ç¡®å®šé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$SCRIPT_DIR"

echo "ğŸ”¥ OntoThink ç‡§åŸT20å®˜æ–¹æ ‡å‡†è®­ç»ƒ"
echo "åŸºäºç‡§åŸå®˜æ–¹chatglm3_6b_1h8c_multiturn.sh"
echo "============================================"

# ============================== ç‡§åŸç¯å¢ƒè®¾ç½® ================================
echo "ğŸ”§ è®¾ç½®ç‡§åŸT20ç¯å¢ƒå˜é‡..."

export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=5
export ECCL_MAX_NCHANNELS=2
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"
export GCU_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

echo "âœ… ç‡§åŸç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ"

# ============================== è®­ç»ƒå‚æ•°é…ç½® ================================
echo "ğŸ“‹ é…ç½®è®­ç»ƒå‚æ•°..."

# æ¨¡å‹è·¯å¾„
export PRETRAINED_MODEL_PATH="$PROJECT_ROOT/enflame_training/models/THUDM/chatglm3-6b"

# è®­ç»ƒæ•°æ®è·¯å¾„
export TRAIN_FILE="$PROJECT_ROOT/enflame_training/datasets/ontothink_multiturn/train.jsonl"

# è®­ç»ƒå‚æ•°ï¼ˆä½¿ç”¨ç‡§åŸå®˜æ–¹æ¨èå€¼ï¼‰
export MAX_TOKENS="2048"
export TP_SIZE="1"
export DP_SIZE="1"
export PP_SIZE="8"
export LADDER_SHAPE="False"
export SKIP_STEPS="10"
export MAX_STEPS="-1"
export MICRO_BATCH_SIZE="1"
export GARDIENT_ACCUMULATION_STEPS="64"
export EVAL_BATCH_SIZE="1"
export EVAL_PER_N_EPOCHS="1"
export TRAIN_EPOCHS="3"

echo "âœ… è®­ç»ƒå‚æ•°é…ç½®å®Œæˆ"

# ============================== æ£€æŸ¥ä¾èµ– ================================
echo "ğŸ” æ£€æŸ¥ç‡§åŸè®­ç»ƒç¯å¢ƒ..."

# æ£€æŸ¥æ¨¡å‹
if [ ! -d "$PRETRAINED_MODEL_PATH" ]; then
    echo "âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: $PRETRAINED_MODEL_PATH"
    exit 1
fi

# æ£€æŸ¥è®­ç»ƒæ•°æ®
if [ ! -f "$TRAIN_FILE" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: $TRAIN_FILE"
    exit 1
fi

# æ£€æŸ¥ç‡§åŸè®­ç»ƒè„šæœ¬
ENFLAME_SCRIPT_DIR=""
for potential_dir in \
    "$PROJECT_ROOT/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3" \
    "/installer/topsrider_extracted/TopsRider_installer/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3" \
    "/usr/local/topsrider/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"; do
    if [ -d "$potential_dir" ] && [ -f "$potential_dir/finetune_chatglm3_for_multiturn.py" ]; then
        ENFLAME_SCRIPT_DIR="$potential_dir"
        break
    fi
done

if [ -z "$ENFLAME_SCRIPT_DIR" ]; then
    echo "âŒ æœªæ‰¾åˆ°ç‡§åŸChatGLM3è®­ç»ƒè„šæœ¬"
    exit 1
fi

echo "âœ… ç‡§åŸè„šæœ¬ç›®å½•: $ENFLAME_SCRIPT_DIR"

# ============================== è¾“å‡ºç›®å½•è®¾ç½® ================================
OUTPUT_DIR="$PROJECT_ROOT/enflame_training/models/ontothink-chatglm3-6b"
mkdir -p "$OUTPUT_DIR"

echo "ğŸ“ è¾“å‡ºç›®å½•: $OUTPUT_DIR"

# ============================== å¯åŠ¨è®­ç»ƒ ================================
echo ""
echo "ğŸš€ å¯åŠ¨OntoThinkç‡§åŸT20è®­ç»ƒ..."
echo "è®­ç»ƒé…ç½®ï¼š"
echo "  æ¨¡å‹: $PRETRAINED_MODEL_PATH"
echo "  æ•°æ®: $TRAIN_FILE"
echo "  æœ€å¤§é•¿åº¦: $MAX_TOKENS"
echo "  å¹¶è¡Œ: TP=$TP_SIZE, DP=$DP_SIZE, PP=$PP_SIZE"
echo "  æ‰¹æ¬¡: micro=$MICRO_BATCH_SIZE, accum=$GARDIENT_ACCUMULATION_STEPS"
echo "  è½®æ•°: $TRAIN_EPOCHS"
echo "  è¾“å‡º: $OUTPUT_DIR"
echo ""

# åˆ‡æ¢åˆ°ç‡§åŸè„šæœ¬ç›®å½•
cd "$ENFLAME_SCRIPT_DIR"

# ä½¿ç”¨ç‡§åŸå®˜æ–¹å¯åŠ¨æ–¹å¼
python3.8 -u -m torch.distributed.launch \
    --nproc_per_node=8 \
    --standalone \
    --use_env finetune_chatglm3_for_multiturn.py \
    --model_path "$PRETRAINED_MODEL_PATH" \
    --train_file "$TRAIN_FILE" \
    --tp_size $TP_SIZE \
    --dp_size $DP_SIZE \
    --pp_size $PP_SIZE \
    --train_micro_batch_size $MICRO_BATCH_SIZE \
    --gradient_accumulation_steps $GARDIENT_ACCUMULATION_STEPS \
    --max_steps $MAX_STEPS \
    --max_tokens $MAX_TOKENS \
    --ladder_shape $LADDER_SHAPE \
    --skip_steps $SKIP_STEPS \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --eval_per_n_epochs $EVAL_PER_N_EPOCHS \
    --train_epochs $TRAIN_EPOCHS

echo "ğŸ‰ è®­ç»ƒå®Œæˆï¼"
