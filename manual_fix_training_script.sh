#!/bin/bash
# æ‰‹åŠ¨ä¿®å¤ç‡§åŽŸè®­ç»ƒè„šæœ¬çš„è·¯å¾„é—®é¢˜

echo "ðŸ”§ æ‰‹åŠ¨ä¿®å¤ç‡§åŽŸè®­ç»ƒè„šæœ¬è·¯å¾„é—®é¢˜"
echo "========================================"

SCRIPT_FILE="enflame_training/scripts/ontothink_chatglm3_enflame.sh"

if [ ! -f "$SCRIPT_FILE" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: $SCRIPT_FILE"
    exit 1
fi

echo "ðŸ“ å¤‡ä»½åŽŸå§‹è„šæœ¬..."
cp "$SCRIPT_FILE" "${SCRIPT_FILE}.backup.$(date +%Y%m%d_%H%M%S)"

echo "ðŸ”§ åº”ç”¨è·¯å¾„ä¿®å¤..."

# æ£€æŸ¥è„šæœ¬æ˜¯å¦å·²ç»åŒ…å«å¤šè·¯å¾„æŸ¥æ‰¾é€»è¾‘
if grep -q "CHATGLM3_SCRIPT_DIRS" "$SCRIPT_FILE"; then
    echo "âœ… è„šæœ¬å·²åŒ…å«å¤šè·¯å¾„æŸ¥æ‰¾é€»è¾‘"
else
    echo "âš ï¸  è„šæœ¬éœ€è¦æ‰‹åŠ¨ä¿®å¤"
    
    # åˆ›å»ºä¿®å¤åŽçš„è„šæœ¬å†…å®¹
    cat > temp_fixed_script.sh << 'EOF'
#!/bin/bash
#
# OntoThink ChatGLM3-6B ç‡§åŽŸT20è®­ç»ƒè„šæœ¬
# åŸºäºŽç‡§åŽŸå®˜æ–¹ChatGLM3å¾®è°ƒè„šæœ¬ä¼˜åŒ–
#
# Copyright (c) 2024 OntoThink Project. All rights reserved.
#

set -eu -o pipefail

# ============================== ç‡§åŽŸT20çŽ¯å¢ƒé…ç½® ================================
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=5
export ECCL_MAX_NCHANNELS=2
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"

# ============================== OntoThinkè®­ç»ƒé…ç½® ================================
# åŸºç¡€è·¯å¾„ - è‡ªåŠ¨æ£€æµ‹
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ENFLAME_ROOT="$(dirname "$SCRIPT_DIR")"
ONTOTHINK_ROOT="$(dirname "$ENFLAME_ROOT")"

echo "ðŸ“ é¡¹ç›®æ ¹ç›®å½•: $ONTOTHINK_ROOT"
echo "ðŸ“ ç‡§åŽŸç›®å½•: $ENFLAME_ROOT"

# æ¨¡åž‹é…ç½®
export PRETRAINED_MODEL_PATH=${PRETRAINED_MODEL_PATH:-"${ENFLAME_ROOT}/models/THUDM/chatglm3-6b"}
export TRAIN_FILE=${TRAIN_FILE:-"${ENFLAME_ROOT}/datasets/ontothink_multiturn/train.jsonl"}

# è®­ç»ƒå‚æ•°é…ç½®
export MAX_TOKENS=${MAX_TOKENS:-"2048"}  # OntoThinkéœ€è¦è¾ƒé•¿ä¸Šä¸‹æ–‡
export TP_SIZE=${TP_SIZE:-"1"}           # å¼ é‡å¹¶è¡Œå¤§å°
export DP_SIZE=${DP_SIZE:-"1"}           # æ•°æ®å¹¶è¡Œå¤§å°  
export PP_SIZE=${PP_SIZE:-"8"}           # æµæ°´çº¿å¹¶è¡Œå¤§å°
export MICRO_BATCH_SIZE=${MICRO_BATCH_SIZE:-"1"}           # å¾®æ‰¹æ¬¡å¤§å°
export GARDIENT_ACCUMULATION_STEPS=${GARDIENT_ACCUMULATION_STEPS:-"64"}  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
export MAX_STEPS=${MAX_STEPS:-"1000"}    # æœ€å¤§è®­ç»ƒæ­¥æ•°
export LADDER_SHAPE=${LADDER_SHAPE:-"false"}  # æ˜¯å¦ä½¿ç”¨é˜¶æ¢¯å½¢çŠ¶
export SKIP_STEPS=${SKIP_STEPS:-"0"}     # è·³è¿‡æ­¥æ•°
export EVAL_BATCH_SIZE=${EVAL_BATCH_SIZE:-"1"}     # è¯„ä¼°æ‰¹æ¬¡å¤§å°
export EVAL_PER_N_EPOCHS=${EVAL_PER_N_EPOCHS:-"1"} # æ¯Nä¸ªepochè¯„ä¼°ä¸€æ¬¡
export TRAIN_EPOCHS=${TRAIN_EPOCHS:-"3"} # è®­ç»ƒè½®æ•°

# è¾“å‡ºç›®å½•é…ç½®
export OUTPUT_DIR=${OUTPUT_DIR:-"${ENFLAME_ROOT}/models/ontothink-chatglm3-6b"}
export LOG_DIR=${LOG_DIR:-"${ENFLAME_ROOT}/logs"}

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p $OUTPUT_DIR
mkdir -p $LOG_DIR

# ============================== çŽ¯å¢ƒæ£€æŸ¥ ================================
echo "ðŸš€ OntoThink ChatGLM3-6B ç‡§åŽŸT20è®­ç»ƒ"
echo "==============================================="
echo "ðŸ“‹ è®­ç»ƒé…ç½®:"
echo "   - åŸºç¡€æ¨¡åž‹: $PRETRAINED_MODEL_PATH"
echo "   - è®­ç»ƒæ•°æ®: $TRAIN_FILE"
echo "   - æœ€å¤§åºåˆ—é•¿åº¦: $MAX_TOKENS"
echo "   - å¹¶è¡Œé…ç½®: TP=$TP_SIZE, DP=$DP_SIZE, PP=$PP_SIZE"
echo "   - æ‰¹æ¬¡é…ç½®: micro_batch=$MICRO_BATCH_SIZE, grad_accum=$GARDIENT_ACCUMULATION_STEPS"
echo "   - è®­ç»ƒè½®æ•°: $TRAIN_EPOCHS"
echo "   - è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "==============================================="

# æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
if [ ! -d "$PRETRAINED_MODEL_PATH" ]; then
    echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°ChatGLM3-6Bæ¨¡åž‹ï¼Œè¯·å…ˆä¸‹è½½åˆ°: $PRETRAINED_MODEL_PATH"
    echo "ðŸ’¡ ä¸‹è½½å‘½ä»¤:"
    echo "   cd $ENFLAME_ROOT/models"
    echo "   git clone https://huggingface.co/THUDM/chatglm3-6b"
    exit 1
fi

# æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨
if [ ! -f "$TRAIN_FILE" ]; then
    echo "âŒ é”™è¯¯: æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: $TRAIN_FILE"
    echo "ðŸ’¡ è¯·å…ˆå‡†å¤‡è®­ç»ƒæ•°æ®"
    exit 1
fi

# ============================== å¯åŠ¨è®­ç»ƒ ================================
echo "ðŸ”¥ å¯åŠ¨OntoThink ChatGLM3è®­ç»ƒ..."

# æŸ¥æ‰¾å¹¶åˆ‡æ¢åˆ°ç‡§åŽŸChatGLM3è„šæœ¬ç›®å½•
CHATGLM3_SCRIPT_DIRS=(
    "${ONTOTHINK_ROOT}/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"
    "/installer/topsrider_extracted/TopsRider_installer/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"
    "${ONTOTHINK_ROOT}/FromEnflame/distributed/llm_scripts_1.0.40/finetuning/chatglm3"
    "${ENFLAME_ROOT}/llm_scripts/finetuning/chatglm3"
)

SCRIPT_DIR_FOUND=""
for dir in "${CHATGLM3_SCRIPT_DIRS[@]}"; do
    if [ -d "$dir" ]; then
        SCRIPT_DIR_FOUND="$dir"
        echo "âœ… æ‰¾åˆ°ChatGLM3è„šæœ¬ç›®å½•: $dir"
        break
    fi
done

if [ -z "$SCRIPT_DIR_FOUND" ]; then
    echo "âŒ æœªæ‰¾åˆ°ChatGLM3è„šæœ¬ç›®å½•ï¼Œè¯·æ£€æŸ¥ç‡§åŽŸå·¥å…·åŒ…å®‰è£…"
    echo "ðŸ” æŸ¥æ‰¾ç‡§åŽŸå·¥å…·åŒ…ç›®å½•ç»“æž„:"
    find "${ONTOTHINK_ROOT}/FromEnflame" -name "*chatglm*" -type d 2>/dev/null | head -5
    exit 1
fi

cd "$SCRIPT_DIR_FOUND"

# æ£€æŸ¥è®­ç»ƒè„šæœ¬æ˜¯å¦å­˜åœ¨
TRAIN_SCRIPT="finetune_chatglm3_for_multiturn.py"
if [ ! -f "$TRAIN_SCRIPT" ]; then
    echo "âŒ æœªæ‰¾åˆ°è®­ç»ƒè„šæœ¬: $TRAIN_SCRIPT"
    echo "ðŸ“‚ å½“å‰ç›®å½•å†…å®¹:"
    ls -la
    echo "ðŸ” æŸ¥æ‰¾ChatGLM3ç›¸å…³è„šæœ¬:"
    find . -name "*chatglm*" -name "*.py" 2>/dev/null
    exit 1
fi

# å¤‡ä»½åŽŸå§‹è®­ç»ƒè„šæœ¬
if [ ! -f "finetune_chatglm3_for_multiturn_original.py" ]; then
    cp "$TRAIN_SCRIPT" "finetune_chatglm3_for_multiturn_original.py"
    echo "ðŸ’¾ å·²å¤‡ä»½åŽŸå§‹è®­ç»ƒè„šæœ¬"
fi

# æ£€æµ‹Pythonå‘½ä»¤
PYTHON_CMD="python3"
if command -v python3.8 &> /dev/null; then
    PYTHON_CMD="python3.8"
fi

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
LOG_FILE="${LOG_DIR}/ontothink_training_$(date +%Y%m%d_%H%M%S).log"

$PYTHON_CMD -u -m torch.distributed.launch \
    --nproc_per_node=8 \
    --standalone \
    --use_env finetune_chatglm3_for_multiturn.py \
    --model_path $PRETRAINED_MODEL_PATH \
    --train_file $TRAIN_FILE \
    --tp_size $TP_SIZE \
    --dp_size $DP_SIZE \
    --pp_size $PP_SIZE \
    --train_micro_batch_size $MICRO_BATCH_SIZE \
    --gradient_accumulation_steps $GARDIENT_ACCUMULATION_STEPS \
    --max_steps $MAX_STEPS \
    --max_tokens $MAX_TOKENS \
    --ladder_shape $LADDER_SHAPE \
    --skip_steps  $SKIP_STEPS \
    --eval_batch_size $EVAL_BATCH_SIZE \
    --eval_per_n_epochs $EVAL_PER_N_EPOCHS \
    --train_epochs $TRAIN_EPOCHS \
    2>&1 | tee "$LOG_FILE"

# æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
if [ ${PIPESTATUS[0]} -eq 0 ]; then
    echo "ðŸŽ‰ OntoThink ChatGLM3è®­ç»ƒå®Œæˆï¼"
    echo "ðŸ“ æ¨¡åž‹è¾“å‡ºç›®å½•: $OUTPUT_DIR"
    echo "ðŸ“Š è®­ç»ƒæ—¥å¿—: $LOG_FILE"
    
    # åˆ›å»ºæ¨¡åž‹ä¿¡æ¯æ–‡ä»¶
    cat > "$OUTPUT_DIR/model_info.json" << EOF
{
    "model_name": "OntoThink-ChatGLM3-6B",
    "base_model": "THUDM/chatglm3-6b",
    "training_data": "$TRAIN_FILE",
    "training_config": {
        "max_tokens": $MAX_TOKENS,
        "tp_size": $TP_SIZE,
        "dp_size": $DP_SIZE,
        "pp_size": $PP_SIZE,
        "micro_batch_size": $MICRO_BATCH_SIZE,
        "gradient_accumulation_steps": $GARDIENT_ACCUMULATION_STEPS,
        "max_steps": $MAX_STEPS,
        "train_epochs": $TRAIN_EPOCHS
    },
    "training_completed": "$(date -Iseconds)",
    "output_directory": "$OUTPUT_DIR"
}
EOF
    
    echo "âœ… è®­ç»ƒä¿¡æ¯å·²ä¿å­˜åˆ°: $OUTPUT_DIR/model_info.json"
    
else
    echo "âŒ OntoThink ChatGLM3è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—: $LOG_FILE"
    exit 1
fi
EOF

    # æ›¿æ¢åŽŸå§‹è„šæœ¬
    mv temp_fixed_script.sh "$SCRIPT_FILE"
    chmod +x "$SCRIPT_FILE"
    
    echo "âœ… è„šæœ¬ä¿®å¤å®Œæˆï¼"
fi

echo ""
echo "ðŸŽ‰ ä¿®å¤å®Œæˆï¼çŽ°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒäº†ï¼š"
echo "   python3 enflame_training/scripts/train_ontothink_enflame.py --step full"
