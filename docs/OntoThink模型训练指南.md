# OntoThink æ¨¡å‹è®­ç»ƒæŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨8å¡GCUèµ„æºè®­ç»ƒOntoThinkä¸“ç”¨çš„ChatGLM3-6Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸“é—¨ç”¨äºç”Ÿæˆä¸­æ–‡å“²å­¦æ€è¾¨å›¾è°±ã€‚

## ğŸ“‹ è®­ç»ƒæ–¹æ¡ˆ

### ğŸ”§ æŠ€æœ¯æ ˆ
- **åŸºç¡€æ¨¡å‹**: ChatGLM3-6B (é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–)
- **è®­ç»ƒæ–¹æ³•**: QLoRA + åˆ†å¸ƒå¼è®­ç»ƒ
- **ç¡¬ä»¶è¦æ±‚**: 8å¡ GCU
- **è®­ç»ƒæ¡†æ¶**: PyTorch + Transformers + PEFT

### ğŸ“Š èµ„æºé…ç½®
- **GPUé…ç½®**: 8å¡å¹¶è¡Œè®­ç»ƒ
- **å†…å­˜éœ€æ±‚**: æ¯å¡çº¦12GBæ˜¾å­˜
- **é¢„è®¡è®­ç»ƒæ—¶é—´**: 3-5å¤©
- **æ•°æ®è§„æ¨¡**: 2000+ é«˜è´¨é‡æ ·æœ¬

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# æ¿€æ´»Pythonç¯å¢ƒ
cd /Users/barryzhang/myDev3/OntoThink_V4
source backend/venv/bin/activate

# å®‰è£…è®­ç»ƒä¾èµ–
pip install torch torchvision torchaudio
pip install transformers>=4.35.0
pip install peft>=0.6.0
pip install datasets
pip install bitsandbytes
pip install accelerate
pip install tensorboard
pip install scikit-learn
pip install aiohttp
```

### 2. é…ç½®APIå¯†é’¥

```bash
# è®¾ç½®DeepSeek APIå¯†é’¥ï¼ˆç”¨äºæ•°æ®æ‰©å±•ï¼‰
export DEEPSEEK_API_KEY="your_deepseek_api_key"
```

### 3. ä¸€é”®è®­ç»ƒ

```bash
# è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
python backend/scripts/train_manager.py --step full --config backend/config/training_config.json
```

## ğŸ“ è¯¦ç»†æ­¥éª¤

### æ­¥éª¤1: æ•°æ®æ‰©å±•
```bash
# ä½¿ç”¨DeepSeek APIæ‰©å±•è®­ç»ƒæ•°æ®
python backend/scripts/expand_training_data.py \
    --api_key $DEEPSEEK_API_KEY \
    --num_samples 300 \
    --output_path backend/data/expanded_data.jsonl
```

### æ­¥éª¤2: æ•°æ®ä¼˜åŒ–
```bash
# ä¼˜åŒ–æ•°æ®æ ¼å¼ï¼Œé€‚é…ChatGLM3è®­ç»ƒ
python backend/scripts/prepare_optimized_data.py \
    --input_dir backend/data/processed \
    --output_dir backend/data/optimized
```

### æ­¥éª¤3: å¯åŠ¨è®­ç»ƒ
```bash
# 8å¡åˆ†å¸ƒå¼è®­ç»ƒ
bash backend/scripts/train_ontothink_8gpu.sh
```

### æ­¥éª¤4: æ¨¡å‹éªŒè¯
```bash
# éªŒè¯è®­ç»ƒåçš„æ¨¡å‹
python backend/scripts/validate_model.py \
    --model_path models/chatglm3-ontothink \
    --test_data_path backend/data/optimized/test.jsonl \
    --output_path models/chatglm3-ontothink/validation_results.json
```

## âš™ï¸ è®­ç»ƒé…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®
```json
{
  "model": {
    "base_model": "THUDM/chatglm3-6b",
    "max_seq_length": 2048,
    "output_dir": "models/chatglm3-ontothink"
  }
}
```

### è®­ç»ƒå‚æ•°
```json
{
  "training": {
    "num_gpus": 8,
    "batch_size_per_gpu": 2,
    "gradient_accumulation_steps": 4,
    "num_epochs": 3,
    "learning_rate": 5e-5,
    "use_lora": true,
    "lora_r": 64,
    "lora_alpha": 128,
    "q_lora": true
  }
}
```

### æ•°æ®é…ç½®
```json
{
  "data": {
    "expand_samples": 300,
    "test_size": 0.1,
    "val_size": 0.1
  }
}
```

## ğŸ“Š æ•°æ®æ ¼å¼è¯´æ˜

### è¾“å…¥æ ¼å¼ (JSONL)
```json
{
  "instruction": "ä½ æ˜¯OntoThinkæ€è¾¨åŠ©æ‰‹...",
  "input": "é—®é¢˜ï¼šäººå·¥æ™ºèƒ½æ˜¯å¦èƒ½å¤ŸçœŸæ­£ç†è§£è¯­è¨€ï¼Ÿ",
  "output": "è®ºæ®ï¼š\n- äººå·¥æ™ºèƒ½é€šè¿‡æ·±åº¦å­¦ä¹ ...",
  "category": "å“²å­¦æ€è¾¨-ç«‹åœºè®ºæ®",
  "task_type": "argument_generation"
}
```

### è¾“å‡ºæ ¼å¼ (æ€è¾¨å›¾è°±JSON)
```json
{
  "question": "äººå·¥æ™ºèƒ½æ˜¯å¦èƒ½å¤ŸçœŸæ­£ç†è§£è¯­è¨€ï¼Ÿ",
  "standpoints": [
    {
      "id": "standpoint_1",
      "text": "äººå·¥æ™ºèƒ½èƒ½å¤ŸçœŸæ­£ç†è§£è¯­è¨€",
      "arguments": [
        {
          "id": "argument_1_1",
          "text": "æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿæ•æ‰è¯­è¨€çš„è¯­ä¹‰ç»“æ„"
        }
      ]
    }
  ],
  "counter_questions": [
    {
      "id": "counter_question_1",
      "text": "ç†è§£è¯­è¨€æ˜¯å¦éœ€è¦æ„è¯†çš„å‚ä¸ï¼Ÿ"
    }
  ]
}
```

## ğŸ” è®­ç»ƒç›‘æ§

### TensorBoardç›‘æ§
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir logs/training --port 6006
```

### å…³é”®æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: åº”é€æ­¥ä¸‹é™è‡³ < 1.0
- **éªŒè¯æŸå¤±**: ä¸åº”å‡ºç°æ˜æ˜¾è¿‡æ‹Ÿåˆ
- **ç”Ÿæˆè´¨é‡**: JSONæ ¼å¼æ­£ç¡®ç‡ > 85%
- **æ€è¾¨æ·±åº¦**: ç«‹åœºå¯¹ç«‹æ€§å’Œè®ºè¯ä¸¥è°¨æ€§

## ğŸ¯ ä¼˜åŒ–å»ºè®®

### 1. æ•°æ®è´¨é‡ä¼˜åŒ–
- ç¡®ä¿ç«‹åœºå¯¹ç«‹æ€§æ˜ç¡®
- è®ºæ®å…·æœ‰å“²å­¦æ·±åº¦
- åé—®å…·æœ‰å¯å‘æ€§
- JSONæ ¼å¼ä¸¥æ ¼æ­£ç¡®

### 2. è®­ç»ƒå‚æ•°è°ƒä¼˜
- **å­¦ä¹ ç‡**: 5e-5 (å¯è°ƒæ•´è‡³ 3e-5 æˆ– 1e-4)
- **LoRA rank**: 64 (å¯è°ƒæ•´è‡³ 32 æˆ– 128)
- **æ‰¹æ¬¡å¤§å°**: æ ¹æ®æ˜¾å­˜è°ƒæ•´
- **è®­ç»ƒè½®æ•°**: 3è½® (å¯æ ¹æ®æ”¶æ•›æƒ…å†µè°ƒæ•´)

### 3. ç¡¬ä»¶ä¼˜åŒ–
- ä½¿ç”¨bf16ç²¾åº¦è®­ç»ƒ
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä¼˜åŒ–æ•°æ®åŠ è½½pipeline
- åˆç†è®¾ç½®å¹¶è¡Œç­–ç•¥

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### è®­ç»ƒæŒ‡æ ‡
- **æ”¶æ•›æ—¶é—´**: 2-3å¤©
- **æœ€ç»ˆæŸå¤±**: < 0.8
- **éªŒè¯å‡†ç¡®ç‡**: > 90%
- **JSONæ ¼å¼æ­£ç¡®ç‡**: > 95%

### ç”Ÿæˆè´¨é‡
- èƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–æ€è¾¨å›¾è°±
- ç«‹åœºè§‚ç‚¹å…·æœ‰å“²å­¦æ·±åº¦
- è®ºæ®æ”¯æ’‘é€»è¾‘ä¸¥è°¨
- åé—®å…·æœ‰å¯å‘æ€§

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. æ˜¾å­˜ä¸è¶³**
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
--per_device_train_batch_size 1
--gradient_accumulation_steps 8
```

**2. è®­ç»ƒé€Ÿåº¦æ…¢**
```bash
# æ£€æŸ¥NCCLé…ç½®
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
```

**3. æ•°æ®æ ¼å¼é”™è¯¯**
```bash
# é‡æ–°è¿è¡Œæ•°æ®ä¼˜åŒ–
python backend/scripts/prepare_optimized_data.py
```

**4. æ¨¡å‹ä¸æ”¶æ•›**
```bash
# è°ƒæ•´å­¦ä¹ ç‡
--learning_rate 3e-5
```

## ğŸ“š å‚è€ƒèµ„æº

- [ChatGLM3 å®˜æ–¹æ–‡æ¡£](https://github.com/THUDM/ChatGLM3)
- [PEFT è®­ç»ƒæŒ‡å—](https://github.com/huggingface/peft)
- [QLoRA è®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [åˆ†å¸ƒå¼è®­ç»ƒæœ€ä½³å®è·µ](https://pytorch.org/tutorials/distributed/ddp_tutorial.html)

## ğŸ‰ è®­ç»ƒå®Œæˆå

### 1. æ¨¡å‹é›†æˆåˆ°åç«¯
```python
# åœ¨FastAPIä¸­é›†æˆè®­ç»ƒå¥½çš„æ¨¡å‹
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("THUDM/chatglm3-6b")
model = PeftModel.from_pretrained(model, "models/chatglm3-ontothink")
```

### 2. éƒ¨ç½²æµ‹è¯•
```bash
# å¯åŠ¨åç«¯æœåŠ¡
cd backend
python -m uvicorn app.main:app --reload
```

### 3. æ€§èƒ½è¯„ä¼°
- æ€è¾¨å›¾è°±ç”Ÿæˆé€Ÿåº¦
- JSONæ ¼å¼æ­£ç¡®ç‡
- å“²å­¦å†…å®¹è´¨é‡
- ç”¨æˆ·ä½“éªŒæ»¡æ„åº¦

---

ğŸš€ **ç¥æ‚¨è®­ç»ƒæˆåŠŸï¼å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ã€‚**
