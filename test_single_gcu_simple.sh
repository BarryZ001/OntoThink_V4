#!/bin/bash

# ğŸ”§ ç®€å•çš„å•GCUè®­ç»ƒæµ‹è¯•
# ç»•è¿‡åˆ†å¸ƒå¼å¤æ‚æ€§ï¼Œæµ‹è¯•åŸºæœ¬åŠŸèƒ½
# ===============================

echo "ğŸ”§ ç®€å•çš„å•GCUè®­ç»ƒæµ‹è¯•"
echo "ç»•è¿‡åˆ†å¸ƒå¼å¤æ‚æ€§ï¼Œæµ‹è¯•åŸºæœ¬åŠŸèƒ½"
echo "=============================="

echo ""
echo "ğŸ¯ ç›®æ ‡ï¼š"
echo "- æµ‹è¯•å•ä¸ªGCUæ˜¯å¦èƒ½æ­£å¸¸è®­ç»ƒ"
echo "- ç»•è¿‡8å¡åˆ†å¸ƒå¼çš„å¤æ‚æ€§"
echo "- ç¡®è®¤æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬è®­ç»ƒæµç¨‹"

echo ""
echo "ğŸ”§ è®¾ç½®ç‡§åŸT20å•GCUç¯å¢ƒ"
echo "========================"

# ç‡§åŸT20ç¯å¢ƒå˜é‡ï¼ˆå•GCUï¼‰
export ENFLAME_ENABLE_EFP=true
export ENFLAME_PT_ENABLE_HBM_INPLACE=true
export OMP_NUM_THREADS=5
export ECCL_MAX_NCHANNELS=1
export ENFLAME_UMD_FLAGS="mem_alloc_retry_times=1"
export GCU_VISIBLE_DEVICES=0  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªGCU

echo "âœ… ç‡§åŸå•GCUç¯å¢ƒè®¾ç½®å®Œæˆ"

echo ""
echo "ğŸ” æ£€æŸ¥ç¯å¢ƒ"
echo "============"

echo "ğŸ”¥ å¯ç”¨çš„GCUè®¾å¤‡:"
ls -la /dev/gcu0 2>/dev/null && echo "âœ… GCU0 å¯ç”¨" || echo "âŒ GCU0 ä¸å¯ç”¨"

echo ""
echo "ğŸ“¦ å…³é”®åŒ…æ£€æŸ¥:"
python3 -c "
packages = ['torch', 'ptex', 'transformers', 'sentencepiece']
for pkg in packages:
    try:
        __import__(pkg)
        print(f'âœ… {pkg}: å¯ç”¨')
    except:
        print(f'âŒ {pkg}: ä¸å¯ç”¨')
"

echo ""
echo "ğŸš€ å¯åŠ¨å•GCUè®­ç»ƒæµ‹è¯•"
echo "===================="

# ç‡§åŸè„šæœ¬ç›®å½•
ENFLAME_SCRIPT_DIR="/workspace/code/OntoThink_V4/FromEnflame/ai_development_toolkit/distributed/llm_scripts_1.0.40/finetuning/chatglm3"

if [ ! -f "$ENFLAME_SCRIPT_DIR/finetune_chatglm3_for_multiturn.py" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨"
    exit 1
fi

echo "âœ… æ‰¾åˆ°è®­ç»ƒè„šæœ¬: $ENFLAME_SCRIPT_DIR"
cd "$ENFLAME_SCRIPT_DIR"

echo ""
echo "ğŸ”§ å¯åŠ¨å•è¿›ç¨‹è®­ç»ƒ (ä¸ä½¿ç”¨åˆ†å¸ƒå¼)..."

# ä½¿ç”¨å•è¿›ç¨‹ï¼Œéåˆ†å¸ƒå¼å¯åŠ¨
python3.8 finetune_chatglm3_for_multiturn.py \
    --model_path "/workspace/code/OntoThink_V4/enflame_training/models/THUDM/chatglm3-6b" \
    --train_file "/workspace/code/OntoThink_V4/enflame_training/datasets/ontothink_multiturn/train.jsonl" \
    --tp_size 1 \
    --dp_size 1 \
    --pp_size 1 \
    --train_micro_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --max_steps 3 \
    --max_tokens 512 \
    --ladder_shape False \
    --skip_steps 1 \
    --eval_batch_size 1 \
    --eval_per_n_epochs 1 \
    --train_epochs 1 \
    --local_rank 0 \
    --world_size 1 \
    --rank 0 2>&1 | tee /tmp/single_gcu_test.log

echo ""
echo "ğŸ” æµ‹è¯•ç»“æœåˆ†æ"
echo "================"

if [ -f /tmp/single_gcu_test.log ]; then
    echo "ğŸ“‹ æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯:"
    if grep -q -i "error\|fail\|exception" /tmp/single_gcu_test.log; then
        echo "âŒ å‘ç°é”™è¯¯:"
        grep -i -A3 -B1 "error\|fail\|exception" /tmp/single_gcu_test.log | tail -10
    else
        echo "âœ… æ²¡æœ‰å‘ç°æ˜æ˜¾é”™è¯¯"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥æ˜¯å¦æˆåŠŸå¼€å§‹è®­ç»ƒ:"
    if grep -q -i "training\|epoch\|step\|loss" /tmp/single_gcu_test.log; then
        echo "âœ… æˆåŠŸå¼€å§‹è®­ç»ƒ!"
        grep -i "training\|epoch\|step\|loss" /tmp/single_gcu_test.log | tail -5
    else
        echo "âŒ æœªèƒ½å¼€å§‹è®­ç»ƒ"
    fi
    
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥æ¨¡å‹åŠ è½½:"
    if grep -q -i "loading\|model\|tokenizer" /tmp/single_gcu_test.log; then
        echo "ğŸ“¦ æ¨¡å‹åŠ è½½ç›¸å…³ä¿¡æ¯:"
        grep -i "loading\|model\|tokenizer" /tmp/single_gcu_test.log | head -5
    fi
fi

echo ""
echo "ğŸ’¡ å•GCUæµ‹è¯•æ€»ç»“"
echo "================="

echo "ğŸ¯ å¦‚æœå•GCUæµ‹è¯•æˆåŠŸï¼š"
echo "  - è¯´æ˜åŸºæœ¬ç¯å¢ƒæ­£å¸¸"
echo "  - é—®é¢˜åœ¨äº8å¡åˆ†å¸ƒå¼é…ç½®"
echo "  - å¯ä»¥å°è¯•å‡å°‘å¹¶è¡Œåº¦ (å¦‚4å¡æˆ–2å¡)"

echo ""
echo "ğŸ¯ å¦‚æœå•GCUæµ‹è¯•å¤±è´¥ï¼š"
echo "  - è¯´æ˜åŸºç¡€ç¯å¢ƒæœ‰é—®é¢˜"
echo "  - éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•æ¨¡å‹åŠ è½½æˆ–å…¶ä»–åŸºç¡€é—®é¢˜"

echo ""
echo "ğŸ“‹ æ—¥å¿—æ–‡ä»¶: /tmp/single_gcu_test.log"
echo "ğŸ” å¯ä»¥æŸ¥çœ‹å®Œæ•´æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯"
